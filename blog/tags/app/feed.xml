<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>twocentstudios</title>
    <description>A coding blog covering iOS, Swift, and other programming topics.</description>
    <link>https://twocentstudios.com/blog/tags/app/index.html</link>
    <atom:link href="https://twocentstudios.com/blog/tags/app/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 30 Nov 2025 02:49:24 -0600</pubDate>
    <lastBuildDate>Sun, 30 Nov 2025 02:49:24 -0600</lastBuildDate>
    <generator>Jekyll v3.9.3</generator>
    
      <item>
        <title>Comprehensible Later: A Read-it-later App for Language Learners</title>
        <description>&lt;p&gt;This post is a short retrospective on Comprehensible Later, my working-title for a read-it-later iOS app prototype I worked on last week. Although it’s currently in private beta on Test Flight, I want to share the motivation and technical challenges I ran into while working on it.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_screens.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Share Extension presented from Safari, main app article list, and article detail screens&quot; title=&quot;Share Extension presented from Safari, main app article list, and article detail screens&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Share Extension presented from Safari, main app article list, and article detail screens&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;TLDR: Comprehensible Later is an iOS app for saving articles natively written in a language you’re learning, with automatic translation via LLM to a simpler version of your target language. The goal is to give you more interesting things to read at a level you can understand without needing to pause to look up every other word.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_japanese_comparison.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Original level and simple level for a Japanese article&quot; title=&quot;Original level and simple level for a Japanese article&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Original level and simple level for a Japanese article&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_english_comparison.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Original level and simple level for an English article (from this blog)&quot; title=&quot;Original level and simple level for an English article (from this blog)&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Original level and simple level for an English article (from this blog)&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-comprehensible-input&quot;&gt;What is Comprehensible Input&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Input_hypothesis&quot;&gt;Comprehensible Input&lt;/a&gt; is part of a language acquisition framework first introduced by &lt;a href=&quot;https://www.sdkrashen.com/&quot;&gt;Dr. Stephen D. Krashen&lt;/a&gt;. The framework states that language is separately &lt;em&gt;acquired&lt;/em&gt; and &lt;em&gt;learned&lt;/em&gt;. &lt;em&gt;Acquisition&lt;/em&gt; happens by ensuring ample input (reading or listening) with the important caveat that the input is &lt;em&gt;comprehensible&lt;/em&gt; at the learner’s current level. &lt;em&gt;Learning&lt;/em&gt; happens through comprehensive study of rules and vocabulary. From this &lt;a href=&quot;https://www.dreaming.com/blog-posts/the-og-immersion-method&quot;&gt;summary&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When we receive comprehensible input, the conditions are met for our brain to be able to use its natural ability to acquire language, without having to do anything else. There’s no need to study, review vocabulary, or practice anything. Watching and reading itself results in acquisition.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In some senses, this method seems intuitive, not in the least since almost all children acquire language skills before they begin formal teaching. In a second-language context, &lt;a href=&quot;https://en.wikipedia.org/wiki/Graded_reader&quot;&gt;graded readers&lt;/a&gt; – books written for various non-native language levels – have existed for over a century. Wikipedia even has a &lt;a href=&quot;https://simple.wikipedia.org/&quot;&gt;simple English&lt;/a&gt; language variant for many common articles. I’ve occasionally used the modern &lt;a href=&quot;https://www.satorireader.com/&quot;&gt;Satori Reader&lt;/a&gt; service for Japanese graded texts.&lt;/p&gt;

&lt;p&gt;But I think the important part is recognizing exactly &lt;em&gt;how basic&lt;/em&gt; you need to make some input in order for it to be understandable, especially at the absolute-beginner level. In a since-removed introductory YouTube video from the creator, he shows a session of an instructor sitting with a zero-level beginner student, pointing at vivid images in a travel magazine and gesturing heavily while explaining the contents very slowly in the target language as the primary means of bootstrapping.&lt;/p&gt;

&lt;p&gt;Language is so multi-dimensional that it’s incredibly time consuming – both as a creator &lt;em&gt;and&lt;/em&gt; a consumer of materials – to get the exact level of material that is both comprehensible but challenging enough to increase your overall ability. Then add another dimension of &lt;em&gt;motivation&lt;/em&gt;: as a reader, how do you find materials with a subject matter that’s interesting to you and will keep you motivated to push through word-after-word, page-after-page, day-after-day?&lt;/p&gt;

&lt;p&gt;This led to a hypothesis:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Graded readers are usually close to the correct difficulty to facilitate learning, but do not have an audience wide enough to support a variety of interesting subject material.&lt;/li&gt;
  &lt;li&gt;Native materials cover an infinite range of interesting topics, but are infeasible to read until the latest stages of language acquisition.&lt;/li&gt;
  &lt;li&gt;One of the most commonly accepted use-cases for LLMs is text summary and translation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;What if we used LLMs to translate any native article on-demand to the user’s exact target language level?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The barriers to this being feasible are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Can an LLM properly translate from e.g. Native Japanese to JLPT N4 level Japanese in a “natural” way – where it is simultaneously challenging, comprehensible, and accurate?&lt;/li&gt;
  &lt;li&gt;Can the translation happen fast enough to fit within a user’s desired language-learning workflow?&lt;/li&gt;
  &lt;li&gt;What additional resources are required to facilitate language learning? In-line dictionary lookup? An SRS system? Customized word lists?&lt;/li&gt;
  &lt;li&gt;What unique points are there to each target language that increase the interface complexity? For example, for Japanese learning, should we include furigana for all potentially unknown kanji?&lt;/li&gt;
  &lt;li&gt;Does it also make sense to allow translation from e.g. Native English to simple Japanese (if the target language is Japanese)?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;specification-for-the-prototype-app&quot;&gt;Specification for the prototype app&lt;/h2&gt;

&lt;p&gt;I’ve used 2025 frontier LLMs to write and translate simple versions of text before, so I’m confident it’s either possible now at a minimally acceptable translation quality or will be in the very near future.&lt;/p&gt;

&lt;p&gt;What I wasn’t confident about is whether it’s cost prohibitive or time prohibitive to use the highest quality reasoning models to do the translation.&lt;/p&gt;

&lt;p&gt;In retrospect, I should have spent at least a little more time doing bench testing on the API versions of various models on a wide array of sample articles. Instead, I took the less (more?) pragmatic route of jumping into the implementation for an app prototype that I could start using ASAP in context, as well as distribute to a few friends.&lt;/p&gt;

&lt;p&gt;My initial thought was that my main source of content would be blog posts and news articles I come across from my everyday feed scrolling. But I also felt I should support translating raw text too, like that from social media posts.&lt;/p&gt;

&lt;p&gt;I considered a Safari Extension to replace existing text on a webpage with the simplified translation, similar to how the built-in translation function in Safari works. But my gut-feeling was that this would be too limiting for language learning use cases. Even reading a text at a simpler level of a target language still takes enough time that it would be better to ensure the user doesn’t feel obligated to read everything at once. Additionally, this wouldn’t work for native text outside of Safari.&lt;/p&gt;

&lt;p&gt;My next thought was a Share Extension. Share Extensions are old iOS technology, but still highly used and useful. In a share extension I could display the translated article content in a dedicated modal and have full control over its presentation and layout.&lt;/p&gt;

&lt;p&gt;However, I also wanted to support the read-it-later use case. Personally, I stumble upon articles when doing feed scrolling sessions when I have a few minutes on the train but don’t necessarily have the time to read the whole article, even in English, at that time. I use Instapaper for read-it-later for English articles and I felt this would be a similarly useful use case to model my app after.&lt;/p&gt;

&lt;p&gt;With that in mind I got to work on the actual prototype with the following initial spec:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A native app that:
    &lt;ul&gt;
      &lt;li&gt;keeps a list of articles imported from URLs or as raw text.&lt;/li&gt;
      &lt;li&gt;has a detail view that shows both the original and translated versions of the text.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A share extension that:
    &lt;ul&gt;
      &lt;li&gt;immediately processes the shared URL or text and displays it in the share modal.&lt;/li&gt;
      &lt;li&gt;allows the user to optionally save the translated text in the app for later.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that as of iOS 18.4, there’s actually &lt;em&gt;another&lt;/em&gt; option for the interface: &lt;a href=&quot;https://developer.apple.com/documentation/TranslationUIProvider/Preparing-your-app-to-be-the-default-translation-app&quot;&gt;TranslationUIProviderExtension&lt;/a&gt;. iOS users can replace Apple’s Translation app with another translation app, meaning the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Translate&lt;/code&gt; menu tooltip can open a third-party app. I have mine set to &lt;a href=&quot;https://www.deepl.com/&quot;&gt;DeepL&lt;/a&gt;. Due to the limitations I’ll discuss later (namely, translation processing time), it doesn’t make sense yet to implement Comprehensible Later as a Translation Extension.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;I worked through several iterations of a detailed implementation spec with Claude and Codex then set them off to work getting the foundations of the app in place. This wasn’t exactly vibe coding because I specified technologies and packages to use up front and guided their output along the way. But I was still aiming to have the agents create the clay that I’d be molding in a distinct second phase of development.&lt;/p&gt;

&lt;h3 id=&quot;packages&quot;&gt;Packages&lt;/h3&gt;

&lt;p&gt;The key packages that would make this closer to a weekend prototype and not a months-long project were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Ryu0118/swift-readability&quot;&gt;swift-readability&lt;/a&gt; - wrapper for Firefox’s &lt;a href=&quot;https://github.com/mozilla/readability&quot;&gt;reader-view parsing library&lt;/a&gt; for stripping down a full page HTML to its essential content.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mattt/AnyLanguageModel&quot;&gt;AnyLanguageModel&lt;/a&gt; - use any LLM API with Apple’s Foundation Models SDK interface.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gonzalezreal/swift-markdown-ui&quot;&gt;swift-markdown-ui&lt;/a&gt; - display the full Markdown spec in SwiftUI (note: I later replaced this).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/steipete/Demark&quot;&gt;Demark&lt;/a&gt; - convert HTML-to-Markdown.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/JohnSundell/Ink&quot;&gt;Ink&lt;/a&gt; - convert Markdown-to-HTML.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pointfreeco/sqlite-data&quot;&gt;sqlite-data&lt;/a&gt; - SQLite wrapper for local article storage and observable data layer for the app.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-flow&quot;&gt;Data flow&lt;/h3&gt;

&lt;p&gt;The initial data flow for articles imported via URL was the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input URL
    ↓ URLSession
HTML Data
    ↓ String(data:)
HTML String
    ↓ Readability
Clean HTML
    ↓ Demark
Markdown
    ↓ AnyLanguageModel
Translated MD
    ↓ swift-markdown-ui
Display
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The initial flow for raw text:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input Text
    ↓ AnyLanguageModel
Translated MD
    ↓ swift-markdown-ui
Display
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;However, due to limitations with the swift-markdown-ui package, the final version of the prototype uses this flow:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input URL
    ↓ URLSession
HTML Data
    ↓ String(data:)
HTML String
    ↓ Readability
Clean HTML
    ↓ Demark
Markdown
    ↓ AnyLanguageModel
Translated MD
    ↓ Ink
HTML
    ↓ WebView
Display
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;With the bones of the app architecture and dependencies in place, I began testing and optimizing the data flow.&lt;/p&gt;

&lt;h3 id=&quot;readability&quot;&gt;Readability&lt;/h3&gt;

&lt;p&gt;I found a small bug in the Readability Swift wrapper where the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;baseURL&lt;/code&gt; parameter was inaccessible to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;URL&lt;/code&gt;-based initializer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Readability().parse(url:options:)&lt;/code&gt;. This prevented relative image tags from getting properly resolved to a full address. For example, on my website image tags look like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/images/example.jpg&lt;/code&gt; and are resolved by my browser automatically to be either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://twocentstudios.com/images/example.jpg&lt;/code&gt; (the real server) or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:4000/images/example.jpg&lt;/code&gt; (my local machine).&lt;/p&gt;

&lt;p&gt;Luckily, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;baseURL&lt;/code&gt; parameter was accessible in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Readability().parse(html:options:baseURL:)&lt;/code&gt; initializer. As a workaround I simply needed to fetch the page data myself with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;URLSession&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;demark&quot;&gt;Demark&lt;/h3&gt;

&lt;p&gt;Demark has two different HTML-&amp;gt;Markdown parsing implementations: heavy-and-accurate or fast-and-inaccurate. Since the HTML is getting pre-processed by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Readability&lt;/code&gt; in advance of being passed to Demark, I’m using the fast-and-inaccurate version that doesn’t load the full page in a headless &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WKWebView&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;anylanguagemodel&quot;&gt;AnyLanguageModel&lt;/h3&gt;

&lt;p&gt;As of iOS 26, Apple’s local Foundation model is slow, not-ubiquitously available on devices, and (arguably) barely functional for most use cases, especially mine. Within a few years I expect it may be useful. Similarly, my impression is that any other MLX-compatible models runnable on an iOS device are not yet accurate or fast enough for my use case.&lt;/p&gt;

&lt;p&gt;Therefore, I grabbed both an OpenAI and Gemini API key and wired them up to AnyLanguageModel for testing. I ran a few trials with the top-tier, mini, and nano variants and decided on defaulting to the mini variant as a compromise between speed, cost, and accuracy. Specifically, Gemini Flash 2.5 is the current default, but I suspect I could spend several weeks creating and running benchmarks across the dozens of closed and open models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mattt/AnyLanguageModel&quot;&gt;AnyLanguageModel&lt;/a&gt; made it easy to build a user settings-based model switcher with very little code adjustments required on my side. Technically, Gemini ships an &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/openai&quot;&gt;OpenAI-compatible endpoint&lt;/a&gt; so I could have kept even more of the same codepath. During debugging, I realized that AnyLanguageModel wasn’t passing through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;instructions&lt;/code&gt; parameter to OpenAI, so I submitted a &lt;a href=&quot;https://github.com/mattt/AnyLanguageModel/pull/20&quot;&gt;quick PR&lt;/a&gt; and Mattt had it merged and version bumped by the next day.&lt;/p&gt;

&lt;p&gt;In a later mini-sprint, I added a full settings screen that allows switching model provider, model, target language, target difficulty, adding custom translation instructions, and even fully rewriting the system prompt. Of course, I would never include all these settings in a production app, but it’s useful for my trusted beta testers to tinker if they so choose.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_settings.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;The translation settings screen&quot; title=&quot;The translation settings screen&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The translation settings screen&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By default, my (simple) system prompt is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Faithfully translate the native-level input markdown text into the target language with the target difficulty level.&lt;/p&gt;

  &lt;p&gt;Be creative in transforming difficult words into simpler phrases that use vocabulary at the target difficulty level. Combine or split sentences when necessary, but try to preserve paragraph integrity.&lt;/p&gt;

  &lt;p&gt;The output format should be standard Markdown including all supported markdown formatting like image/video tags. Preserve all structure from the input (paragraphs, lists, headings, links, images, videos). DO NOT ADD COMMENTARY.&lt;/p&gt;

  &lt;p&gt;Target language: \(targetLanguage)&lt;br /&gt;
Target difficulty level: \(targetDifficulty)&lt;br /&gt;
Additional notes: \(additionalNotes)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I’ll discuss my impressions of the effectiveness of this prompt a little later on.&lt;/p&gt;

&lt;p&gt;Something I noticed almost immediately during testing was that requests were taking at minimum 30 seconds and sometimes over 60 seconds to complete. It didn’t really depend on model size either. I found the same performance characteristics for both OpenAI and Gemini APIs direct from first-party servers. I thought it might be the streaming API or perhaps some configuration in AnyLanguageModel I was not in control of, so I switched back to the single-request version. It didn’t help. I also began testing the same prompt and inputs from the API sandbox pages like &lt;a href=&quot;https://platform.openai.com/chat/edit&quot;&gt;OpenAI’s playground&lt;/a&gt; and &lt;a href=&quot;https://aistudio.google.com/u/1/prompts/new_chat&quot;&gt;Google’s AI Studio&lt;/a&gt; and saw basically the same results.&lt;/p&gt;

&lt;p&gt;Although the slow translation speed is a pretty substantial blocker, I felt like, at least temporarily, I could work around it in the UX by leaning into the read-it-later nature of the app. I added support for Apple’s &lt;a href=&quot;https://developer.apple.com/documentation/backgroundtasks&quot;&gt;Background Tasks&lt;/a&gt; API so there was a greater chance that articles added early in the day would be ready to read by the time the user opened the app.&lt;/p&gt;

&lt;h3 id=&quot;app-ui&quot;&gt;App UI&lt;/h3&gt;

&lt;p&gt;With the translation flow in place, I began shaping the app UI.&lt;/p&gt;

&lt;p&gt;The list of articles was simple enough. I held off on adding lots of important, but not urgent, contextual actions like archiving and deleting from the list view.&lt;/p&gt;

&lt;p&gt;I did add both “import from pasteboard” and “import from free text” buttons to the toolbar.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_article_list.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;The article list view showing toolbar buttons for importing from pasteboard and free text&quot; title=&quot;The article list view showing toolbar buttons for importing from pasteboard and free text&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The article list view showing toolbar buttons for importing from pasteboard and free text&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_import_screen.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;The import URL/text screen&quot; title=&quot;The import URL/text screen&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The import URL/text screen&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I spent more time on the article detail view. Initially, it displayed the title, import state, and translated article. My focus for adding actions was to facilitate debugging primarily for myself and secondarily for my beta testers. This meant buttons for copying the original article text, copying translated article text, deleting an article, opening the original link, and retrying the translation (with different settings).&lt;/p&gt;

&lt;p&gt;After some initial usage, I realized I wanted to see the original text and the translated text side-by-side so that I could compare the language usage by sentence and paragraph.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_article_detail.jpg&quot; width=&quot;&quot; height=&quot;500&quot; alt=&quot;Article detail view with options to display original and translated text&quot; title=&quot;Article detail view with options to display original and translated text&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Article detail view with options to display original and translated text&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_article_actions.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Supported actions for article detail&quot; title=&quot;Supported actions for article detail&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Supported actions for article detail&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_debug_info.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Debug info viewer&quot; title=&quot;Debug info viewer&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Debug info viewer&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, the most time-consuming and impactful change was the markdown display system. This was a tough decision, but I think ultimately necessary for the first version.&lt;/p&gt;

&lt;p&gt;Originally, I was planning to use &lt;a href=&quot;https://github.com/gonzalezreal/swift-markdown-ui&quot;&gt;swift-markdown-ui&lt;/a&gt; to display the translated markdown text in SwiftUI. This implementation was basically plug-and-play, rendered exactly as I wanted, supported images out of the box, and was performant. However, the &lt;a href=&quot;https://github.com/gonzalezreal/swift-markdown-ui/issues/264&quot;&gt;one fundamental and unsolvable issue&lt;/a&gt; is that &lt;strong&gt;SwiftUI Text only supports paragraph level copy support and does not support character-level or word-level selection&lt;/strong&gt;. For language learning, I absolutely need the ability to select a word and use the context menu tooltip action “Look Up” or “Translate” or “Copy” buttons. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;swift-markdown-ui&lt;/code&gt; would not be able to support this and I needed to research other solutions.&lt;/p&gt;

&lt;p&gt;I spent nearly a full day researching and experimenting with other Markdown solutions. My second preference was to convert Markdown to AttributedString either &lt;a href=&quot;https://developer.apple.com/documentation/foundation/instantiating-attributed-strings-with-markdown-syntax&quot;&gt;natively&lt;/a&gt; or &lt;a href=&quot;https://github.com/madebywindmill/MarkdownToAttributedString&quot;&gt;with a package&lt;/a&gt;, then display the AttributedString in a &lt;a href=&quot;https://github.com/kevinhermawan/SelectableText&quot;&gt;SwiftUI-wrapped UITextView&lt;/a&gt; with selection enabled but editing disabled. However, both the native and package versions of AttributedString initialization failed at properly respecting whitespace, newlines, and supporting images. My estimation was that it’d take significantly more time for me to grok the full Markdown spec, all the underlying packages, and then implement the required patches than I was willing to spend for a prototype.&lt;/p&gt;

&lt;p&gt;Therefore, I pivoted to using a browser-based target view instead. iOS 26 was blessed with &lt;a href=&quot;https://developer.apple.com/documentation/webkit/webview-swift.struct&quot;&gt;WebView&lt;/a&gt;, a modern SwiftUI-native implementation of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UIWebView&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WKWebView&lt;/code&gt; UIKit views before it. With a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WebView&lt;/code&gt; as the new target, I used &lt;a href=&quot;https://github.com/JohnSundell/Ink&quot;&gt;Ink&lt;/a&gt; to convert the LLM output Markdown back to HTML, added a barebones stylesheet, and loaded these contents. I don’t love using a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WebView&lt;/code&gt; for this use case since it’s comparatively heavy, has plenty of rendering quirks (like occasional white background flashes), and requires a full screen layout. But at the moment it’s the least-worst option.&lt;/p&gt;

&lt;h3 id=&quot;share-extension-and-action-extension&quot;&gt;Share Extension and Action Extension&lt;/h3&gt;

&lt;p&gt;Unfortunately, the slow translation speed meant some of the complexity of creating a fully-featured Share Extension was in vain; it didn’t make sense for the user to wait 30-60 seconds for the share extension to load a preview of the article content like I’d originally planned.&lt;/p&gt;

&lt;p&gt;My initial vision was to load a one-page preview of the translation as quickly as possible. Then, I’d allow the user to tap a button to continue viewing the full translation in line. Or at any time they could tap a button to save the article URL (or raw text) to the main app to read later. I was planning on having an “open in app” button too, but as far as I can tell it’s not supported to open an app directly from a Share Extension.&lt;/p&gt;

&lt;p&gt;I kept the full functionality of the share extension intact in case I can solve the translation speed issue in the future. But as another workaround, I added an Action Extension. An Action Extension appears in the bottom section of the system share sheet. Like a Share Extension it can also present custom UI, however since I already have a Share Extension I made my Action Extension have no UI and immediately save the URL to the app.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_share_sheet.jpg&quot; width=&quot;&quot; height=&quot;450&quot; alt=&quot;iOS share sheet showing both the Share Extension and Action Extension for quickly saving articles&quot; title=&quot;iOS share sheet showing both the Share Extension and Action Extension for quickly saving articles&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;iOS share sheet showing both the Share Extension and Action Extension for quickly saving articles&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_share_extension.jpg&quot; width=&quot;&quot; height=&quot;450&quot; alt=&quot;Share Extension with translation complete&quot; title=&quot;Share Extension with translation complete&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Share Extension with translation complete&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;import-flow&quot;&gt;Import flow&lt;/h3&gt;

&lt;p&gt;App Extensions can share data on device with the main app using an &lt;a href=&quot;https://developer.apple.com/documentation/Xcode/configuring-app-groups&quot;&gt;App Group&lt;/a&gt;. When the user indicates they want to add the URL or raw text to the app, the Extension serializes an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Article&lt;/code&gt; model to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;json&lt;/code&gt; and writes a new file to the App Group. The main app monitors the shared App Group directory for new files. When it detects a new file, it adds the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Article&lt;/code&gt; to the app’s SQLite database. If the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Article&lt;/code&gt; already finished translation, it will include the translated markdown and no further processing is necessary. Otherwise, it will be queued for processing.&lt;/p&gt;

&lt;p&gt;I chose not to share the SQLite database between the main app and the extensions because, since the app and extensions are separate processes, there are &lt;a href=&quot;https://swiftpackageindex.com/groue/GRDB.swift/v7.8.0/documentation/grdb/databasesharing&quot;&gt;myriad issues&lt;/a&gt; with using SQLite in this way. Since data sharing is one way (from extension to app) there’s no need to introduce that complexity.&lt;/p&gt;

&lt;p&gt;Adding articles from the main app instance skips the file encoding/decoding step and simply writes a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Article&lt;/code&gt; to the database.&lt;/p&gt;

&lt;p&gt;The processing code is admittedly a bit fragile, but in testing has worked well enough that I haven’t felt an immediate need to rewrite it. It uses an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enum Status&lt;/code&gt; stored alongside each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Article&lt;/code&gt; in the database in order to manage the translation queue, including failures. &lt;a href=&quot;https://github.com/pointfreeco/sqlite-data&quot;&gt;SQLiteData&lt;/a&gt; supports observation, so both the article list view and the article detail view are always up to date on an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Article&lt;/code&gt;’s status.&lt;/p&gt;

&lt;h3 id=&quot;localization&quot;&gt;Localization&lt;/h3&gt;

&lt;p&gt;Localizing a prototype would be something I’d never consider doing before the advent of coding agents. The actual act of translation between a base language and another language is insignificant compared to the amount of additional tooling and operational complexity of introducing localization keys, adding comments, handling interpolation, handling pluralization rules, handling error messages and other strings generated deep in business logic, and handling the indirection involved in looking up the values for the keys. The new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xcstrings&lt;/code&gt; file’s autogeneration definitely helps ease the burden. But it’s at least an order of magnitude more work in my opinion.&lt;/p&gt;

&lt;p&gt;All that said, coding agents can automate enough of this work that I added full localization support for Japanese for one of my beta testers who wanted to try the app for converting English to simple English. I’m still cognizant of the ongoing support complexity full localization adds to a prototype, but for now it’s not a decision I regret.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/comprehensible_later_japanese_localization.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Various screens with Japanese localization&quot; title=&quot;Various screens with Japanese localization&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Various screens with Japanese localization&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;impressions-so-far&quot;&gt;Impressions so far&lt;/h3&gt;

&lt;p&gt;What I’ve learned so far is that the prompt needs to be more customized to each target language and should probably go as far as including an allow-list of words to use, especially for the most basic target difficulties.&lt;/p&gt;

&lt;p&gt;I’ve found the models have a hard time with native Japanese news articles. Something about the language is just so dense that my first prompt attempt does not push the model to simplify enough.&lt;/p&gt;

&lt;p&gt;Similar to what I’ve found with even commercial apps like Instapaper, a large percentage of sites now have enough paywall or otherwise reader-hostile javascript that it’s not enough to fetch a simple URL directly from the source. I’m not ready to handle the endless, unforgiving work of handling all the edge cases of the open web, so URL fetching is going to be best effort for the foreseeable future.&lt;/p&gt;

&lt;p&gt;The Readability library itself is not perfect at parsing out text from pages that aren’t obviously written as “articles”. This isn’t all that different from the built-in Safari reader mode which isn’t universally supported across the entire web.&lt;/p&gt;

&lt;p&gt;Seeing some of my blog posts in super-simple English was really fun. One of my ongoing goals is to write simpler without giving up my voice, so seeing how an LLM breaks up my sentences and phrases and clauses is enlightening (of course, not at all related to the use case the prototype was built for).&lt;/p&gt;

&lt;p&gt;For Japanese, there’s some unpredictability on how the LLM deals with kanji. Usually it includes kanji as is, but sometimes it will add the reading in parentheses directly after for literally every word. For example, “果物（くだもの）を食べる（たべる）”. Native ruby/furigana support would be ideal, and possibly easier using HTML than &lt;a href=&quot;https://github.com/ApolloZhu/RubyAttribute&quot;&gt;AttributedString&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Comprehensible Later is on Test Flight in private beta with myself and a few friends. I’m planning on collecting feedback and evaluating the app’s potential for wider release. It could take another generation or two of LLM. It could take as long as waiting for local models to improve. Or the entire concept could be flawed. I’m not sure yet. But that’s what the prototype is for.&lt;/p&gt;

&lt;p&gt;Regardless of the result, it was of course a good learning experience to see what it’s like to build a read-it-later service for iOS in 2025.&lt;/p&gt;
</description>
        <pubDate>Sat, 15 Nov 2025 05:57:31 -0600</pubDate>
        <link>https://twocentstudios.com/2025/11/15/comprehensible-later-read-it-later-for-language-learners/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2025/11/15/comprehensible-later-read-it-later-for-language-learners/</guid>
        
        <category>apple</category>
        
        <category>ios</category>
        
        <category>comprehensiblelater</category>
        
        <category>app</category>
        
        
      </item>
    
      <item>
        <title>Vibe Coding a Rental Apartment Search Management App</title>
        <description>&lt;p&gt;I’ve been apartment hunting here in the Tokyo-area with my girlfriend. We’ve been sending links to various rental property listings back and forth in LINE (messaging app) and emailing with brokers. In a chat interface, it was hard keeping up with the status of each of the properties we’d seen, we wanted to see, we’d inquired about, etc. Classic project management problem.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-suumo-listing-example.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Example SUUMO property listing page slightly edited for clarity&quot; title=&quot;Example SUUMO property listing page slightly edited for clarity&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Example SUUMO property listing page slightly edited for clarity&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I decided to skip the step of putting each property into a shared spreadsheet and jump straight to vibe coding a web app with Claude Code. I’ve never worked on a full-stack TypeScript app before, and my impression is that LLMs are most proficient at it, so that’s what I went with.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-finished-desktop-interface.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Finished desktop interface for Bukkenlist&quot; title=&quot;Finished desktop interface for Bukkenlist&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Finished desktop interface for Bukkenlist&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My goal was to create a shared space where we could keep track of the status of listings, add new ones easily, do calculations like 2-year amortized cost, keep a notes and ratings field for each of us, see all the salient points of a property at a glance, and archive properties that we decide against or are already taken.&lt;/p&gt;

&lt;p&gt;After a day of work, it supported scraping SUUMO listings and worked on mobile and desktop web. Another 2 half-days of work and it supports 4 listing sites, maps, expired listings, and English/Japanese localization. I called it Bukkenlist 物件リスト.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-day1-and-final-comparison.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;App progression from end of day 1 (left) to final polished version (right) - there&apos;s not much visual difference&quot; title=&quot;App progression from end of day 1 (left) to final polished version (right) - there&apos;s not much visual difference&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;App progression from end of day 1 (left) to final polished version (right) - there&apos;s not much visual difference&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This was “vibe coding” in the customary definition of “not looking at the generated code at all”. I see the code scrolling past in the terminal window but I’m letting Claude commit it after I check that the rendered result looks and works as intended in the browser window. For this project, I’m playing the role of product manager and QA engineer. However, I did make the decisions about using SQLite for storage, the schema, and the deployment strategy. And I helped Claude dig itself out of holes in the way only an engineer can.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The TL;DR:&lt;/strong&gt; In 2 working days I produced a completely functional web app with much better usability than the spreadsheet it would compete with. Using an AI tool like Claude Code aimed at professionals, it’s hard for me to imagine someone with no coding background being able to get to the same finish line I did. But with the existing cadre of no-code AI tools, perhaps this would be a perfectly scoped project.&lt;/p&gt;

&lt;h2 id=&quot;the-full-development-process&quot;&gt;The full development process&lt;/h2&gt;

&lt;p&gt;I have a Claude Code $100/mo Max subscription. I used the pattern of using “plan mode” with Opus aggressively to ensure proper context gathering and then “accept edits” mode with Sonnet to execute the plan. These were long sessions, so I actually blew through my usage limits once or twice with 1 or 2 hours remaining (with my usual Swift projects I hadn’t hit the Max limit for Sonnet before). At those times, I switched over to the nascent OpenAI Codex CLI (with a $20/mo Pro plan) to see how it did. Everything about Codex still feels months behind Claude Code, but it did handle some of the tasks I threw at it well enough.&lt;/p&gt;

&lt;h3 id=&quot;day-1&quot;&gt;Day 1&lt;/h3&gt;

&lt;p&gt;Learning from some &lt;a href=&quot;/2025/06/22/vinylogue-swift-rewrite/&quot;&gt;past experiments&lt;/a&gt;, I decided this time to be more intentional with my initial getting-started prompts. I didn’t dump my entire vision for the app onto the model and have it create a full product requirements doc and phase-by-phase development plan. I thought staying in the loop would ensure the best chance of success and even minor scalability.&lt;/p&gt;

&lt;p&gt;SUUMO listing scraping was the most risky part, so I had it start by creating some infrastructure around fetching and parsing the HTML for a few example listings and comparing the results with the values I’d plucked out by hand.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-console-parsing-results.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Console showing results from the initial SUUMO listing parsing&quot; title=&quot;Console showing results from the initial SUUMO listing parsing&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Console showing results from the initial SUUMO listing parsing&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Only after the parsing seemed relatively robust did I have Claude create the initial structure of the Express.js backend and React frontend. It used VITE but I only sort of know what role that plays. The first renderable version was a text field for the SUUMO URL, a submit button, and then a list of the keys and values parsed out.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-first-working-interface.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;First working interface with URL input field and parsed listing key/values&quot; title=&quot;First working interface with URL input field and parsed listing key/values&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;First working interface with URL input field and parsed listing key/values&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then it was time to add persistence. This was the part where I &lt;em&gt;should&lt;/em&gt; have first decided on hosting, got that set up, &lt;em&gt;then&lt;/em&gt; decided on the most low maintenance storage solution. Instead, I chose SQLite, which I’ve been interested in lately and have &lt;a href=&quot;/2025/07/02/swift-vapor-fly-io-sqlite-config/&quot;&gt;already deployed&lt;/a&gt; successfully on Fly.io.&lt;/p&gt;

&lt;p&gt;With my engineer hat on, I made the initial decision to have Claude go with a mixed schema-less approach, storing a generous amount of metadata about each property in named columns, but then having a dumping ground JSON column with all the parsed key/value data. Hard to say whether that’s made my life easier or harder while adding new listing sources. For a personal project with 2 users, I think it was a fine decision. For a real production site, I can already tell it would be a nightmare to maintain.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Final schema of the `scrapes` table
     cid  name           type     notnull  dflt_value  pk
     ---  -------------  -------  -------  ----------  --
     0    id             INTEGER  0                    1
     1    url            TEXT     1                    0
     2    property_name  TEXT     0                    0
     3    scraped_data   TEXT     1                    0
     4    created_at     INTEGER  1                    0
     5    status         TEXT     0                    0
     6    archived       INTEGER  1        0           0
     7    kiyoko_notes   TEXT     0                    0
     8    chris_notes    TEXT     0                    0
     9    kiyoko_rating  INTEGER  0                    0
     10   chris_rating   INTEGER  0                    0
     11   source_site    TEXT     1        &apos;suumo&apos;     0
     12   color_id       TEXT     0                    0
     13   latitude       REAL     0                    0
     14   longitude      REAL     0                    0
     15   expired        INTEGER  1        0           0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;From there I had Claude build out the master/detail list in desktop mode. It had little trouble putting together a passible design.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-initial-master-detail-view.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Initial master-detail interface showing property list and selected property detail&quot; title=&quot;Initial master-detail interface showing property list and selected property detail&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Initial master-detail interface showing property list and selected property detail&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I added delete and refresh support since those were helpful in manual testing. Refresh should re-run the scraping and parsing and replace all the fields with the freshly parsed content.&lt;/p&gt;

&lt;p&gt;Then it was kind of the fun part: pushing around the fields in the UI to make it more pretty and readable.&lt;/p&gt;

&lt;p&gt;Next I had to add image carousel support which was surprisingly easy. I prompted Claude to do some extra research for best practices before deciding on a solution.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-ui-reorganizing-carousel.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Reorganized UI with image carousel&quot; title=&quot;Reorganized UI with image carousel&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Reorganized UI with image carousel&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I added deterministic unique color generation for each property based on its unique ID mapped to a hue value 0-359 in HSV. I use this technique often in projects as a nice touch to make resources easier to identify.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-color-id-support.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Properties with unique color IDs for intuitive identification&quot; title=&quot;Properties with unique color IDs for intuitive identification&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Properties with unique color IDs for intuitive identification&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I can’t sightread Japanese as fast as I can English, so I had Claude add full UI localization in both English and Japanese to the entire app and have it save the preference in local storage. This helped speed up QA of parser errors going forward.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-localization-support.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;English/Japanese localization toggle&quot; title=&quot;English/Japanese localization toggle&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;English/Japanese localization toggle&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I wanted to highlight the at-a-glance parts each property that were especially important to the two of us, so I added those in big font next to the image carousel.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-at-a-glance-properties.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;First version of the at-a-glance property details&quot; title=&quot;First version of the at-a-glance property details&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;First version of the at-a-glance property details&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From here it was a lot of polish. I felt like I was in full product manager flow-state, just picking off the next obvious change in the UI and prompting Claude to have a go at it.&lt;/p&gt;

&lt;p&gt;The whole point of the app was to facilitate our apartment search process, which ultimately meant appending our own information to listings. I added an open-ended status field to track things like “requested viewing” or “viewing on 8/24”. I added an open-ended notes field for each of us, then a 4-level rating system. In the notes field, we’ve been adding merits/demerits. The rating system is an easy way to clearly communicate our enthusiasm towards each property and see it at a glance in the sidebar.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-notes-and-ratings.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;First version of the notes and rating system for each (hardcoded) user&quot; title=&quot;First version of the notes and rating system for each (hardcoded) user&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;First version of the notes and rating system for each (hardcoded) user&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A cool feature I’d very loosely prototyped with a single ChatGPT query a few days previously was an “amortized cost” field, calculated from several fields. There are so many disparate fees for each listing (monthly rent, management fees, security deposit, key money, parking fee, etc.) that it’s hard to do an apples-to-apples comparison of how expensive properties actually are. It’s elementary school math, but just annoying to do.&lt;/p&gt;

&lt;p&gt;It was pretty simple to add this field: parse out the semantic values, multiply the monthly costs by the lease term, add the one-time costs, then divide by the lease term to get the overall monthly cost.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-all-in-cost.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Amortized cost calculation for true monthly expense comparison&quot; title=&quot;Amortized cost calculation for true monthly expense comparison&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Amortized cost calculation for true monthly expense comparison&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I was on the fence about whether to build out a full user table and authentication system. It may have been worth seeing whether Claude could have one-shotted multi-user support. Instead, I opted for a simple password auth and full editing support for any field. I’m pretty happy with this solution and proud of myself for not going overboard on the spec. It’s much easier to share a single password than deal with a create account flow on multiple devices or while on the go. I set some strict rate limits for password attempts and page requests in general and know that if the site gets hacked and trashed somehow it’s not a huge deal.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-login-screen.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Simple password login screen to gate the whole app&quot; title=&quot;Simple password login screen to gate the whole app&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Simple password login screen to gate the whole app&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It was finally time for deployment! I was definitely procrastinating on this, but I wanted to get it online before I went to bed.&lt;/p&gt;

&lt;p&gt;Looking into some of the common free hosting services that target JS, I realized Vercel was serverless and I’d need a different solution for the SQLite storage. I could have tried Turso for SQLite hosting, but signing up for 2 services felt like too much complexity. I went back to Fly.io since I have some experience with them and an existing account and all the CLI stuff installed.&lt;/p&gt;

&lt;p&gt;Claude was happy to set up all the deployment stuff and mostly one-shotted it. The big issue came with my underlying scraping implementation. Scraping was based on &lt;a href=&quot;https://playwright.dev/&quot;&gt;Playwright&lt;/a&gt; which needs to spin up a full Chromium instance and that takes 10+ seconds on a 2 GB machine. I have aggressive suspension set for my Fly.io instances which means this heavy startup cost needs to be paid every time a new listing is added. I also didn’t want to pay for a full 2 GB machine on Fly.&lt;/p&gt;

&lt;p&gt;I started another vibe spike to replace Playwright with Puppeteer and a lighter Chromium fork based on &lt;a href=&quot;https://vercel.com/guides/deploying-puppeteer-with-nextjs-on-vercel&quot;&gt;this guide&lt;/a&gt; from Vercel. With a lot of trial and error (including rewriting the parser), I got the memory requirement down to 512 MB at the cost of 30+ second scraping.&lt;/p&gt;

&lt;p&gt;At this point, I took a step back and thought about whether I actually needed a full Chromium-based scraper. After all, I’d never actually verified whether these sites were doing enough JS rendering to require it. I don’t have a lot of experience with scrapers and this project was an attempt to fix that. I had Claude do yet another spike with some initial research as to what the most common tools were for low-resource scraping and it chose &lt;a href=&quot;https://github.com/jsdom/jsdom&quot;&gt;JSDOM&lt;/a&gt;. After rewriting the parser yet again, it turned out this worked fine and was super fast and easily deployable to a tiny 256 MB machine.&lt;/p&gt;

&lt;p&gt;If I’d have tried deploying immediately after finishing the very first version of the scraper, I’d have had a much easier time. But I also realized I wouldn’t have had much invested at this point, and my motivation to continue may not have survived this deployment slog. An interesting paradox! In theory I would have saved hours, but in practice I may not have shipped anything. It’s also possible I would have chosen a different deployment service that affected my choice of persistence solution, etc. Decision ordering really matters, and I’m trying to get better at it. But also, LLMs make spikes, backtracking, and rewrites so low-cost/low-effort that as long as you’re willing to ignore sunk costs and your motivation survives you can end up with much more optimal solutions in the long run.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-deployed-production-end-day1.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;App successfully deployed to production at the end of day 1 (Japanese interface)&quot; title=&quot;App successfully deployed to production at the end of day 1 (Japanese interface)&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;App successfully deployed to production at the end of day 1 (Japanese interface)&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My first commit was 4pm on Saturday and my last commit before going to sleep was 6am Sunday. I’d (re)watched 3 seasons of Silicon Valley in the background. I sent my girlfriend a link and the password and went to bed.&lt;/p&gt;

&lt;h2 id=&quot;day-2--3&quot;&gt;Day 2 &amp;amp; 3&lt;/h2&gt;

&lt;p&gt;I woke up a couple hours later and made pancakes and got a message from my girlfriend with links to listings from 2 other services. So it was time to add support for more listing sources!&lt;/p&gt;

&lt;p&gt;I had Claude do a refactor of the scraper in preparation for adding multi-service support. Again, this was vibe coding so I had no idea how well it did, but I trusted it. This took about an hour. I gave it a link to a new listing and had it run its scraping parsing iterative procedure to write an initial version of the parser. According to the git logs it took about an hour to write the two new scrapers and a guide for itself for writing future scrapers.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-multi-source-support.jpg&quot; width=&quot;&quot; height=&quot;300&quot; alt=&quot;Multi-source support showing listings from different rental websites&quot; title=&quot;Multi-source support showing listings from different rental websites&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Multi-source support showing listings from different rental websites&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I realized the two of us would need to use my new site from our iPhones, so I added mobile support. This was way way faster than I expected. It took a bit more Claude coercing the next day to get it fully optimized, but the first attempt was definitely usable.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-initial-mobile-interface.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Initial mobile interface with separate property list screen (left) and property detail screen (right)&quot; title=&quot;Initial mobile interface with separate property list screen (left) and property detail screen (right)&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Initial mobile interface with separate property list screen (left) and property detail screen (right)&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sunday was a half-day. On Monday, I put in another half-day optimizing the mobile layout, adding another source, adding listing expiry support, maps support, and deep linking support.&lt;/p&gt;

&lt;p&gt;Maps support was actually the most difficult single-feature I’d vibe coded for the whole project. Scraping the coordinates for a listing wasn’t too bad, but deciding on the maps provider and implementation was difficult.&lt;/p&gt;

&lt;p&gt;At first, I was planning on rendering out a static map image on the backend during property add because it seemed simplest and lowest cost. But since I already have an Apple Developer account, using the MapKit JS API was free so I went with that.&lt;/p&gt;

&lt;p&gt;Turns out that Claude had a pretty awful time integrating the MapKit JS library. This is where I ran into a lot of frustration with vibe coding and not having any idea how React works, how JS library loading should work, how environment variables work on the client side, how JS library token authorization should work, and more. I was in thrashing mode with Claude, watching it implement “fixes” that seemed dubious even to me, a JS novice, and inevitably did not work at all.&lt;/p&gt;

&lt;p&gt;I had to get a lot more hands on and spent a long dev cycle restarting the dev server, deploying to production over and over, copying and pasting browser console logs, and adding and removing secrets from the Fly.io admin page.&lt;/p&gt;

&lt;p&gt;In the end, we got it working, but it’s hard for me to say &lt;em&gt;why&lt;/em&gt; Claude struggled so hard with this particular task and how I could have approached it differently.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/bukkenlist-maps-support.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Map showing the property location and its relationship to closest station&quot; title=&quot;Map showing the property location and its relationship to closest station&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Map showing the property location and its relationship to closest station&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;iteration-loop-workflow&quot;&gt;Iteration loop workflow&lt;/h2&gt;

&lt;p&gt;For my overall development experience, the local build and serve process was more effortless than iOS, but I found Claude’s new background Bash processes feature frustrating.&lt;/p&gt;

&lt;p&gt;Claude would write some code, start a background server, test the code by making some calls to the server, make some code changes, then not realize that the server needed to be restarted and get stuck in a “why isn’t the output changing” loop. I’d need to keep an eye out for this and intervene.&lt;/p&gt;

&lt;p&gt;After a while I took control of starting/stopping the dev server in a separate terminal window, but Claude would ignore this and keep trying to do its own thing. If I was working on this full time I would certainly spend some time making this flow more efficient. I dealt with the paper cuts.&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;We still have at least a few weeks left in the apartment hunting process. The less we need to use this app the happier I will be.&lt;/p&gt;

&lt;p&gt;For the time investment, I’d consider this app overkill. It’s useful individually, but for a production app it wouldn’t work as scraping is presumably against the TOSes. The parsing is sloppy. There’s no user account system. There’s no sharing system. There’s no base SaaS functionality. Even individually, it would have taken a lot less time to simply enter the key fields into a spreadsheet manually.&lt;/p&gt;

&lt;p&gt;But this was a good experience seeing how feasible vibe coding is for someone with my background. There were several points in the process where I hit that beautiful flow state and really loved it. But there were also stretches where Claude was thrashing and I was losing my patience. Or when I was the tool of the LLM clicking boxes in admin panels or doing visual QA while it was doing the interesting architecture and coding tasks.&lt;/p&gt;

&lt;p&gt;One thing’s for certain: I never would have attempted a project like this without an LLM agent. If I did, I probably would have lost motivation after finishing the first scraper. I probably would have used a technology I already knew even if it was not the most prudent choice in 2025.&lt;/p&gt;

&lt;p&gt;I’d like to try some of the more “batteries included” vibe coding environments (e.g. Lovable, Bolt, Replit, V0) to do a similarly scoped project in the near future. I’m most comfortable with Claude Code at the moment because I’m used to the freedom + sharp edges combination. But it’s hard for me to imagine a non-programmer or even a junior programmer being able to dig themselves out of the holes I found myself in a few times. There’s just a &lt;em&gt;lot&lt;/em&gt; to know still to get an MVP designed, developed, and deployed.&lt;/p&gt;

&lt;p&gt;I can see how using a vibe coding environment with less freedom but more well-paved integrations could prevent dead-ends and thrashing and bad developer experience. Maybe within the year both Claude Code and the vibe coding platforms will have converged into providing decent enough support for users of any background.&lt;/p&gt;

</description>
        <pubDate>Mon, 18 Aug 2025 12:46:25 -0500</pubDate>
        <link>https://twocentstudios.com/2025/08/18/vibe-coding-a-rental-apartment-search-management-app/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2025/08/18/vibe-coding-a-rental-apartment-search-management-app/</guid>
        
        <category>claude</category>
        
        <category>web</category>
        
        <category>react</category>
        
        <category>typescript</category>
        
        <category>vibecoding</category>
        
        <category>app</category>
        
        <category>javascript</category>
        
        <category>nodejs</category>
        
        <category>flyio</category>
        
        <category>sqlite</category>
        
        <category>debugging</category>
        
        
      </item>
    
      <item>
        <title>Reintroducing Technicolor: Binge Watch with Friends Over Space and Time</title>
        <description>&lt;p&gt;Although it’s still in beta, I think it’s a good time to reintroduce my web app side project called Technicolor.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-beta-icon.png&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Technicolor beta app icon&quot; title=&quot;Technicolor beta app icon&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Technicolor beta app icon&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Technicolor is a chat app tailored for watching TV shows with friends asynchronously. I’ve found it to be a great way to stay in touch with friends in other cities/states/countries.&lt;/p&gt;

&lt;p&gt;The current version of Technicolor is a native SwiftUI app available on iOS 17.4+ devices and macOS.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-beta-overview.png&quot; width=&quot;800&quot; height=&quot;&quot; alt=&quot;Technicolor app overview showing dashboard, room interface (redacted), and media inspector screens&quot; title=&quot;Technicolor app overview showing dashboard, room interface (redacted), and media inspector screens&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Technicolor app overview showing dashboard, room interface (redacted), and media inspector screens&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;how-it-works&quot;&gt;How it works&lt;/h2&gt;

&lt;p&gt;Technicolor is a multi-player experience.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Gather some friends&lt;/strong&gt;: Technicolor is optimized for IRL close friends. A group of 2-4 friends is most optimal.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Choose a TV show to watch&lt;/strong&gt;: The best picks are shows that you can imagine generating a lot of reactions or hot takes, but choosing the right show is more art than science in my experience so far. Technicolor doesn’t embed/include media, so all members must have access to a streaming service or file-based media.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Create a Room&lt;/strong&gt;: Each episode discussion lives in its own chat room. Technicolor uses &lt;a href=&quot;https://www.themoviedb.org/&quot;&gt;TMDB&lt;/a&gt; as its media metadata provider.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Watch the episode on your own schedule&lt;/strong&gt;: Each Room member watches the episode and leaves comments tagged with the timestamp. If you watch first, you lay the foundation for discussion. If you watch later, you’re often replying to existing comments. It’s a unique experience each way.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mark as watched&lt;/strong&gt;: Tap the “Mark as Watched” button to alert other members via push notification that you’ve finished watching so they can read your comments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Read and reply&lt;/strong&gt;: Respond to comments from other members. Keep the discussion going for as many turns as you’d like.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Move onto the next episode&lt;/strong&gt;: Technicolor is media-aware and has helpers for quickly creating a Room for the next episode in the current or next season.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;main-features&quot;&gt;Main features&lt;/h2&gt;

&lt;h3 id=&quot;navigate-your-watchlist-in-the-dashboard&quot;&gt;Navigate your watchlist in the Dashboard&lt;/h3&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-beta-dashboard.png&quot; width=&quot;600&quot; height=&quot;&quot; alt=&quot;Technicolor Dashboard showing active rooms organized by TV show&quot; title=&quot;Technicolor Dashboard showing active rooms organized by TV show&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Technicolor Dashboard showing active rooms organized by TV show&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Manage your watchlist on the Dashboard screen. The dashboard intelligently manages active Rooms so you have quick access to episodes you need to watch and those you need to read comments for.&lt;/p&gt;

&lt;p&gt;Rooms are grouped logically by TV show and members.&lt;/p&gt;

&lt;p&gt;Technicolor also supports movie watching groups. All movies watched by the same members are grouped in one section.&lt;/p&gt;

&lt;p&gt;It’s easy to create the next episode by tapping the more button and choosing “Create Room for S03E01”.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-beta-create-next-room.png&quot; width=&quot;600&quot; height=&quot;&quot; alt=&quot;Create Room for next episode popup&quot; title=&quot;Create Room for next episode popup&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Create Room for next episode popup&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;leave-comments-in-a-room&quot;&gt;Leave comments in a Room&lt;/h3&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-beta-room-interface.png&quot; width=&quot;600&quot; height=&quot;&quot; alt=&quot;Room interface showing timestamped comments (redacted)&quot; title=&quot;Room interface showing timestamped comments (redacted)&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Room interface showing timestamped comments (redacted)&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Comments in a Room are grouped into mini-threads by timestamp.&lt;/p&gt;

&lt;p&gt;There’s a custom control for selecting a timestamp by tapping and dragging like a video scrubber.&lt;/p&gt;

&lt;video src=&quot;/images/technicolor-beta-timestamp-control.mov&quot; controls=&quot;&quot; preload=&quot;none&quot; poster=&quot;/images/technicolor-beta-timestamp-control-poster.png&quot; height=&quot;600&quot;&gt;&lt;/video&gt;

&lt;p&gt;Tap the info button to see a quick overview of metadata about the episode via TMDB.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-beta-info-button.png&quot; width=&quot;600&quot; height=&quot;&quot; alt=&quot;Episode info screen showing metadata from TMDB&quot; title=&quot;Episode info screen showing metadata from TMDB&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Episode info screen showing metadata from TMDB&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;start-a-new-show&quot;&gt;Start a new show&lt;/h3&gt;

&lt;p&gt;There’s a flow for adding your first Room for a show. Search for a TV Show, Movie, or enter a custom title. Then select which friends you’ll watch with.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-beta-new-room-search.png&quot; width=&quot;600&quot; height=&quot;&quot; alt=&quot;New Room search screen showing Breaking Bad search results&quot; title=&quot;New Room search screen showing Breaking Bad search results&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;New Room search screen showing Breaking Bad search results&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;friend-management&quot;&gt;Friend management&lt;/h3&gt;

&lt;p&gt;Technicolor has a full mutual-friend management system. You can only create new Rooms with users you have a mutual friendship with (or are already in an existing group with).&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-beta-me-screen.png&quot; width=&quot;600&quot; height=&quot;&quot; alt=&quot;Me screen showing friendship management and settings&quot; title=&quot;Me screen showing friendship management and settings&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Me screen showing friendship management and settings&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;invites&quot;&gt;Invites&lt;/h3&gt;

&lt;p&gt;In this beta phase, Technicolor uses an invite system to control new user sign-ups. A user can create unlimited invite codes to invite their IRL friends.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-beta-invites.png&quot; width=&quot;600&quot; height=&quot;&quot; alt=&quot;Invite screen for generating invite codes and checking redemption status&quot; title=&quot;Invite screen for generating invite codes and checking redemption status&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Invite screen for generating invite codes and checking redemption status&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;h3 id=&quot;async-only&quot;&gt;Async-only&lt;/h3&gt;

&lt;p&gt;This native-first version of Technicolor follows a few other variants I’ve created and used over the years (see the History section below).&lt;/p&gt;

&lt;p&gt;In earlier versions, Technicolor could operate as both a live-streaming, synchronous client in addition to a async client. I found that live-streaming support, although kind of cool, didn’t really make sense in the timestamp marked format. Normal chat apps work fine since everyone is synced and reading/writing comments in real time. There’s also not much reason to keep the history around since everyone has already caught up on the comments.&lt;/p&gt;

&lt;p&gt;For this reason, and to keep implementation complexity low, I’ve left out Websocket support. Optimizing the UI and UX for asynchronous makes things much simpler to use, explain, and maintain.&lt;/p&gt;

&lt;h3 id=&quot;apple-platforms-only&quot;&gt;Apple platforms-only&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Most&lt;/em&gt; of my friends are iOS/macOS users, and since I am an iOS specialist, I’ve decided to only target Apple platforms for now.&lt;/p&gt;

&lt;p&gt;I’m considering a web version for the future to include my Android/Windows friends. But at the moment, it’s already a lot to fill out the feature set and polish the UX for a non-revenue-generating side project.&lt;/p&gt;

&lt;h2 id=&quot;the-future-of-technicolor&quot;&gt;The future of Technicolor&lt;/h2&gt;

&lt;p&gt;I’m planning to do beta testing with my close friends for a while until all the primary flows of the app feel production ready.&lt;/p&gt;

&lt;p&gt;I still have plenty of unimplemented ideas that could improve the commenting-while-watching flow.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There should be a timer mode that automatically counts up while the episode is running, so you don’t have to choose timestamps manually as often.&lt;/li&gt;
  &lt;li&gt;The TMDB episode detail screen should also load the list of actors and characters in an episode for easy reference. Bonus points for being able to @-mention a character/actor in the chat box with autocomplete support.&lt;/li&gt;
  &lt;li&gt;I’ve implemented subtitle fetching support on the backend, but haven’t thought through exactly how to surface this info in the app. You could optionally attach a line from the subtitles to a new comment. Or subtitles could automatically appear in line near timestamps automatically.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have reservations about releasing a social network to the general public due to the amount of moderation required. Technicolor is closer to a private chat app than a social network. And with its current user relationship model it’s probably safe enough and niche enough that bad actors can’t wreak too much havoc. But I’d still probably need to implement message reporting, an admin dashboard for moderation, and some more safeguards around spamming. Being iOS/macOS-only also helps ensure that spam is less prevalent than it otherwise might be.&lt;/p&gt;

&lt;p&gt;I could keep the invite system to both limit the growth rate, prevent abuse, and contribute to successful user onboarding.&lt;/p&gt;

&lt;p&gt;I’d probably need to harden the backend API a little more, maybe put Cloudflare in front of it. The Fly.io instance currently goes to sleep when there’s no activity to save money, so I could choose to keep it on. The machine specs are very weak, so I could beef those up as well to ensure my users see the theoretical speed of Swift on the server.&lt;/p&gt;

&lt;h2 id=&quot;a-brief-history-of-technicolor&quot;&gt;A brief history of Technicolor&lt;/h2&gt;

&lt;p&gt;I first wrote about Technicolor back in 2013 in &lt;a href=&quot;/2014/01/26/fall-2013-project-wrap-up/#:~:text=to%20show%20though.-,Technicolor%20TV,-Status%3A%20Under%20Infrequent&quot;&gt;a post about my side projects&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-3.png&quot; width=&quot;600&quot; height=&quot;&quot; alt=&quot;Original Technicolor web app dashboard&quot; title=&quot;Original Technicolor web app dashboard&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Original Technicolor web app dashboard&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/technicolor-4.png&quot; width=&quot;600&quot; height=&quot;&quot; alt=&quot;Original Technicolor room interface&quot; title=&quot;Original Technicolor room interface&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Original Technicolor room interface&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The short version is that I moved from Chicago to New York and still wanted a way to watch TV shows with my Chicago friends. We’d still regularly get on video calls for live stream watching, but for other shows we started emailing each other with timestamped comments. These emails started to get unwieldy, and I posited that a simple web app could improve the watching experience while also facilitating episode management outside an email client.&lt;/p&gt;

&lt;p&gt;The first version was implemented as a Ruby on Rails app deployed to Heroku and vending HTML and simple JavaScript. It was browser-only and not well-optimized for mobile devices (at the time, I didn’t own a dedicated TV set and watched most shows on my computer).&lt;/p&gt;

&lt;p&gt;After the initial short development period, Technicolor worked well enough that I didn’t need to do much active development. Additionally, the code I wrote as a hobbyist Rails/JS-dev had already reached the point of complexity where it was no longer easy or safe to make even minor changes.&lt;/p&gt;

&lt;p&gt;In late-2017 I moved to Japan and was motivated again to revive the side project as mobile first. I also wanted to explore a different backend architecture that was type-safe. I briefly started an Elixir backend, and although I already had a little experience with it, it soon became clear that it was too bespoke, especially for a side project I only had occasional time to devote to.&lt;/p&gt;

&lt;p&gt;Swift on the server was starting to get some buzz, and some exploration with Vapor made it seem like there were theoretical merits to having a unified language for back-end and front-end development.&lt;/p&gt;

&lt;p&gt;I started chipping away at development again (while still using the legacy web-app version), but I fell into side-project hell where each time I’d have the motivation to work on it, I’d spend the entire day updating Vapor, migrating Swift, changing hosting providers, or converting to the latest SwiftUI APIs. There was very little forward progress.&lt;/p&gt;

&lt;p&gt;I also fell into various scope-creep traps. Adding the invite system. Adding full mutual friendship support and blocking. My previous episode data provider TVDB switched to a paid-only API, so I descoped that and moved to a Room system that had no media linking.&lt;/p&gt;

&lt;p&gt;Heroku’s free tier was discontinued in 2022, and with it the legacy version of Technicolor went offline.&lt;/p&gt;

&lt;p&gt;Coming off some headwinds with &lt;a href=&quot;/2025/06/22/vinylogue-swift-rewrite/&quot;&gt;my last rewrite experience&lt;/a&gt;, I finally decided to check the status of my codebase after over two years of dormancy. As I suspected, Claude Code made quick work of updating all my iOS code to my preferred architecture and the latest APIs. Unfortunately, the Vapor framework was a lot further behind the Swift 6 migration than I’d hoped, but still has reasonable support for most bread-and-butter web app capabilities.&lt;/p&gt;

&lt;p&gt;I got somewhat lost in the scope creep flow state, tearing through the implementation of all the features that had been rotting on my TODO list for a literal decade. On the user-facing side, I’m especially proud of the very streamlined dashboard layout and the push notifications support (the impetus of the mobile-first rewrite in the first place). On the development tooling side, I’m proud of having a comprehensive server-side test suite and a custom release wizard script that prepares and submits both iOS and macOS versions of the app.&lt;/p&gt;

&lt;h2 id=&quot;tech-stack-details&quot;&gt;Tech stack details&lt;/h2&gt;

&lt;p&gt;As mentioned above, Technicolor’s backend is written using the &lt;a href=&quot;https://vapor.codes/&quot;&gt;Swift Vapor&lt;/a&gt; framework. It’s hosted on &lt;a href=&quot;https://fly.io/&quot;&gt;Fly.io&lt;/a&gt;. The client is an iOS target written in Swift and SwiftUI and supporting iOS 17+. There’s technically a native macOS target via Mac Catalyst, but it actually looks and functions worse than the “Designed for iPad” version, so I’m probably going to deprecate the Mac Catalyst version.&lt;/p&gt;

&lt;p&gt;&lt;del&gt;I’ll do a deeper dive into the tech stack in a future post because I think there’s at least a few interesting and unique points to the architecture.&lt;/del&gt; I go into the technical details in the next post: &lt;a href=&quot;/2025/08/04/full-stack-swift-technicolor-technical-architecture/&quot;&gt;Full-Stack Swift: The Technical Architecture of Technicolor&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Fri, 25 Jul 2025 09:00:00 -0500</pubDate>
        <link>https://twocentstudios.com/2025/07/25/reintroducing-technicolor-binge-watch-with-friends-over-space-and-time/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2025/07/25/reintroducing-technicolor-binge-watch-with-friends-over-space-and-time/</guid>
        
        <category>technicolor</category>
        
        <category>app</category>
        
        
      </item>
    
      <item>
        <title>Eki Live - Zero-Touch Assistant for Navigating Tokyo&apos;s Railways</title>
        <description>&lt;p&gt;I’m excited to announce my latest app Eki Like or 駅ライブ in Japanese.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-live-v1-app-icon.png&quot; width=&quot;&quot; height=&quot;200&quot; alt=&quot;App icon for Eki Live v1.0&quot; title=&quot;App icon for Eki Live v1.0&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;App icon for Eki Live v1.0&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Tokyo-area residents can download it &lt;a href=&quot;https://apps.apple.com/us/app/eki-live/id6745218674&quot;&gt;on the App Store&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Eki Live tracks the train you’re currently on and shows the current/next station on your route.&lt;/p&gt;

&lt;p&gt;The main interface is a Live Activity:&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-live-live-activity-en.png&quot; width=&quot;&quot; height=&quot;300&quot; alt=&quot;Live Activity on the lock screen and Dynamic Island&quot; title=&quot;Live Activity on the lock screen and Dynamic Island&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Live Activity on the lock screen and Dynamic Island&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But there’s also a more detailed view inside the app:&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-live-v1-home-en.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Home screen of Eki Live (English version)&quot; title=&quot;Home screen of Eki Live (English version)&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Home screen of Eki Live (English version)&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The unique point of Eki Live is that it’s designed as a &lt;strong&gt;zero-touch&lt;/strong&gt; app:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The app runs silently (and lightly) in the background throughout the day.&lt;/li&gt;
  &lt;li&gt;When it detects you’re on a train, the app automatically determines which railway line you’re on and which direction you’re headed.&lt;/li&gt;
  &lt;li&gt;The app starts a Live Activity that appears on your lock screen and Dynamic Island (on supported iPhones).&lt;/li&gt;
  &lt;li&gt;When your trip is over, the Live Activity automatically disappears.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All this happens with &lt;em&gt;zero user interaction&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I’ve been using the app during the last few months of development and I think it’s kind of magical. Please &lt;a href=&quot;https://apps.apple.com/us/app/eki-live/id6745218674&quot;&gt;give it a try&lt;/a&gt; and send me your thoughts at &lt;a href=&quot;support@twocentstudios.com?subject=Eki%20Live%20Feedback&quot;&gt;support@twocentstudios.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you’d like a deep dive into Eki Live, why I made it, its limitations, and what I’ve got planned for it next, please keep reading.&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;Eki Live relies primarily on Apple’s Location Services framework – which itself relies mostly on GPS data. Therefore, Eki Live has very limited operation on underground railway lines. This often includes the many stations that are covered or partially underground, as well as tunnels and dense city-areas.&lt;/p&gt;

&lt;p&gt;For efficient background standby operation, Eki Live uses a Location Services API called &lt;em&gt;significant location changes&lt;/em&gt;. The app stays asleep in the background like all other apps on your device, but is awoken by the system when your device moves some distance from its previous location. When awoken, Eki Live briefly checks whether the device is moving at train-speeds, and if not, it goes right back to sleep. This means that Eki Live will wake up and find your current railway at minimum a few hundred meters after you’ve departed your origin station and sometimes longer. The detection time can be random depending on several factors (whether the system is already using your location, your battery level, etc.).&lt;/p&gt;

&lt;p&gt;When GPS accuracy drops significantly, Eki Live will report stations as &lt;strong&gt;Nearby&lt;/strong&gt; instead of &lt;strong&gt;Next&lt;/strong&gt; or &lt;strong&gt;Now&lt;/strong&gt;. When no new GPS coordinates have been delivered by the system in over 60 seconds, Eki Live will switch to reduced accuracy mode with hatching and a low signal indicator shown in the Live Activity and in the app.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-live-v1-reduced-accuracy.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Reduced accuracy mode is triggered when no GPS coordinates have been received in 60 seconds or more&quot; title=&quot;Reduced accuracy mode is triggered when no GPS coordinates have been received in 60 seconds or more&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Reduced accuracy mode is triggered when no GPS coordinates have been received in 60 seconds or more&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As of v1.0, Eki Live will dismiss itself once you’ve stopped moving at train speeds for a period of about 10 minutes. This is only a temporarily limitation and will be improved. If you’re feeling impatient, you can swipe left on the Live Activity on the lock screen to dismiss it.&lt;/p&gt;

&lt;p&gt;Similarly, Eki Live will usually handle above ground transfers within a couple hundred meters of the transfer station, but it depends on how divergent the new railway line is from the previous one. This is also a limitation I think I’ll be able to improve in the near future.&lt;/p&gt;

&lt;p&gt;Differentiating between railway lines that run parallel is difficult for Eki Live’s tracking algorithm without more input than just GPS. At the moment, Eki Live will use data about which stations you’ve stopped at versus which you’ve passed to determine which of up to several parallel railways you’re currently on. But this can take as much time as it takes to get to a few stations. If you’re feeling impatient, you can always tap Eki Live’s Live Activity to open the app and tap another railway candidate.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-live-v1-railway-selection.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;You can open the app to manually select your current railway if it&apos;s not the top candidate; in this example the Meguro-line&quot; title=&quot;You can open the app to manually select your current railway if it&apos;s not the top candidate; in this example the Meguro-line&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;You can open the app to manually select your current railway if it&apos;s not the top candidate; in this example the Meguro-line&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Eki Live does not currently differentiate between local and express trains for railways that have them. However, Eki Live will sometimes show stations as &lt;strong&gt;Passing&lt;/strong&gt; instead of &lt;strong&gt;Now&lt;/strong&gt; when it has high confidence a station will be passed and not stopped at. This depends on many cars the train has and which car (front or back) you are currently occupying.&lt;/p&gt;

&lt;p&gt;Finally, there is an undocumented hard-limit on the number of Live Activities an iOS app can start during a period of time while the app is in the background. As of iOS 18.4, the limit is 10 times per 24-hour window. This means that Eki Live will only be able to &lt;em&gt;start&lt;/em&gt; 10 Live Activities per day from the background. In practice this should be an incredibly rare occurrence. But if it does happen, the workaround would be to open the app, which would start the Live Activity directly on device.&lt;/p&gt;

&lt;p&gt;My mission from version 1.0 onward is to overcome these limitations one-by-one to make the app fulfill its promise of being truly zero-touch on any railway.&lt;/p&gt;

&lt;h2 id=&quot;privacy&quot;&gt;Privacy&lt;/h2&gt;

&lt;p&gt;The goal of Eki Live is to improve your ease of navigation around Tokyo. Eki Live does not and will never have ads or sell data to third-parties.&lt;/p&gt;

&lt;p&gt;Eki Live does not send any raw location data (i.e. latitude, longitude) off device (I have no interest in knowing where you are or where you’ve been).&lt;/p&gt;

&lt;p&gt;The only exception to the above is a technical detail due to an Apple API limitation, which I will explain below.&lt;/p&gt;

&lt;p&gt;In order for the app to start a Live Activity automatically while the app is not open, it must send data to Apple’s push notification server (APNS). APNS then forwards the data back to the device.&lt;/p&gt;

&lt;p&gt;When Eki Live is running in the background and determines which railway line you’re on, it sends (as an example) the following information to a twocentstudios.com server:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  focusStationPhaseKey: &apos;focusStationPhase.upcoming&apos;,
  focusStation: &apos;Jiyugaoka&apos;,
  laterLaterStation: &apos;Gakugei-daigaku&apos;,
  laterStation: &apos;Toritsu-daigaku&apos;,
  railway: &apos;Toyoko Line&apos;,
  railwayDestinationStation: &apos;Shibuya&apos;,
  railwayHexColor: &apos;#DA0442&apos;,
  isReducedAccuracy: false
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The above information is packaged up without any parsing or logging and forwarded directly to APNS. It has only the information required to populate the first state of the Live Activity that will start on your device. The device address keys for APNS are generated anonymously by the operating system and are also not logged or associated with you in any way.&lt;/p&gt;

&lt;p&gt;Only the first set of data must be relayed through APNS in order to start the Live Activity. After the Live Activity successfully starts, it is updated directly on the device without any external server communication.&lt;/p&gt;

&lt;p&gt;I hope this superfluous server-based workaround for the ActivityKit API will be eliminated in the future.&lt;/p&gt;

&lt;p&gt;If for some reason you want to opt-out of any server-based communication:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deny background Location Services permissions (please still permit “when in use” Location Services permissions)&lt;/li&gt;
  &lt;li&gt;Open the app manually each time you’d like to start tracking a journey.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All static data for stations and railways is included when downloading the app and is stored locally on your device. All location-based searching happens locally.&lt;/p&gt;

&lt;h2 id=&quot;battery-impact&quot;&gt;Battery impact&lt;/h2&gt;

&lt;p&gt;Eki Live is carefully designed to minimize battery impact.&lt;/p&gt;

&lt;p&gt;I’m acutely aware that iOS users are extremely protective of battery life, and apps that run in the background are rightfully treated with great suspect.&lt;/p&gt;

&lt;p&gt;Eki Live has two states:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;idle in the background&lt;/li&gt;
  &lt;li&gt;tracking a journey&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In short, in my testing, Eki Live has the following battery impact:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;While idle&lt;/strong&gt; - an unmeasurably small amount of battery impact.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;While actively tracking a journey&lt;/strong&gt; - about the same battery impact as listening to a song on bluetooth headphones locally in the Music app.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’re looking for more detail, I’ll clarify exactly how Eki Live affects battery life below.&lt;/p&gt;

&lt;p&gt;While idle in the background, Eki Live uses zero processing power. It may or may not use some resident memory (an attribute managed by the operating system alongside all other backgrounded apps).&lt;/p&gt;

&lt;p&gt;When iOS detects your device has moved some hundreds of meters, it will wake up Eki Live for a few seconds to provide location data. Eki Live will quickly check whether your device is moving at train speeds. If it’s not, it goes right back to sleep.&lt;/p&gt;

&lt;p&gt;If the device &lt;em&gt;is&lt;/em&gt; moving, Eki Live switches into tracking mode.&lt;/p&gt;

&lt;p&gt;In tracking mode, Eki Live processes new GPS coordinates up to one coordinate per second while moving, and fewer while stopped at stations. Processing involves a few local SQLite database queries and some math operations. When the app is open, the map is updated as new GPS coordinates are received. When the app is closed, the app will only send updates when the contents of the Live Activity change (e.g. the station phase changes from “soon” to “now”).&lt;/p&gt;

&lt;p&gt;The processing work described above still has some room for optimizations, so future releases will include battery life improvements.&lt;/p&gt;

&lt;h2 id=&quot;eki-live-and-ambient-computing&quot;&gt;Eki Live and ambient computing&lt;/h2&gt;

&lt;p&gt;Eki Live is an experiment in &lt;a href=&quot;https://en.wikipedia.org/wiki/Ambient_intelligence&quot;&gt;ambient computing&lt;/a&gt;. Ambient computing is technology that blends into the environment and uses context to respond to human behavior without explicit commands. In the case of Eki Live, the app uses iPhone sensors to determine whether it’s on a train and integrate the sensor data with map data to determine which railway you’re on.&lt;/p&gt;

&lt;p&gt;My friend Sergio and I were at lunch talking about smartphones and apps several years ago. He said something that, at the time, didn’t totally make sense based on how limited iOS was outside of the app ecosystem. Paraphrasing him: “People don’t want to open apps. I should be able to get 90% of what I need from an app without opening it.”&lt;/p&gt;

&lt;p&gt;In the long early years of iOS, push notifications were the only way for an app to escape its bounds and integrate into other parts of the system UI. I was skeptical of Sergio’s sentiment at the time because it felt impossible to achieve. Push notifications are somewhat one-dimensional in what kind of user value you can deliver.&lt;/p&gt;

&lt;p&gt;In the more recent era of iOS, there are many more integration points for apps into the system UI: &lt;a href=&quot;https://support.apple.com/en-us/118610&quot;&gt;Widgets&lt;/a&gt;, &lt;a href=&quot;https://support.apple.com/guide/iphone/use-the-dynamic-island-iph28f50d10d/ios&quot;&gt;Live Activities&lt;/a&gt;, &lt;a href=&quot;https://developer.apple.com/app-clips/&quot;&gt;App Clips&lt;/a&gt;, &lt;a href=&quot;https://support.apple.com/guide/shortcuts/welcome/ios&quot;&gt;Shortcuts&lt;/a&gt;, &lt;a href=&quot;https://support.apple.com/en-us/118232&quot;&gt;Spotlight&lt;/a&gt;, etc. Each of these carves out a new niche and bridges the gap between app UI and system UI.&lt;/p&gt;

&lt;p&gt;Outside the iOS world, there’s been an influx of consumer hardware experiments that push ambient computing in some way: wearables like the defunct &lt;a href=&quot;https://en.wikipedia.org/wiki/Humane_Inc.&quot;&gt;Humane pin&lt;/a&gt;; voice assistants like Alexa speakers; and robot vacuums to name a few. Mixed reality wearables may eventually become ubiquitous and escape the app-centered philosophy still core to the OS of the Apple Vision Pro.&lt;/p&gt;

&lt;p&gt;All this is to say that Eki Live is my own skeptical experiment into escaping the app-bounds. As an app dev, the easiest delusion to fall into is “people want to open my app”. Eki Live is the first app I’ve tried to design around users getting value from the app without needing to open it or even remember it exists.&lt;/p&gt;

&lt;p&gt;The key phrase in that last sentence is “user value”. Showing users exactly what they want before they know they want it is very very hard. And to me, it’s annoying when it’s not right. For example, the “context aware” Widget Stacks built into iOS are hopeless. For all but the most obvious use cases it’s impossible to predict what people want to do with 100% accuracy. The only reason I started working on Eki Live was based on &lt;em&gt;probably&lt;/em&gt; having the ability to predict when someone was riding a train and then being able to present the UI automatically in a non-obtrusive way that still provided enough value.&lt;/p&gt;

&lt;p&gt;And, from a development perspective, it was not easy to connect all those dots. As of version 1.0, the dots are still somewhat loosely connected (as detailed in the Limitations section above):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The app waking up in the background is limited by the accuracy of the Core Location APIs to quickly report significant location changes.&lt;/li&gt;
  &lt;li&gt;The app being able to accurately detect your current railway line is based on the accuracy of GPS and the density of other railways in the area.&lt;/li&gt;
  &lt;li&gt;The amount of information the app can show is limited to the space allotted to Live Activities on the lock screen and Dynamic Island.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If all these parts worked flawlessly, you would always have an indicator of the current/next station inconspicuously located in view. But what is the &lt;em&gt;value&lt;/em&gt; in that?&lt;/p&gt;

&lt;h2 id=&quot;why-do-train-passengers-need-to-see-the-currentnext-station&quot;&gt;Why do train passengers need to see the current/next station?&lt;/h2&gt;

&lt;p&gt;Put simply, when riding a train, you need to know what station you’re at so you can decide &lt;em&gt;whether or not to get off the train&lt;/em&gt;. By nature, the current/next station information is only relevant for a few minutes at a time, and thus your awareness of it as a rider needs to be continuously updated.&lt;/p&gt;

&lt;p&gt;Something I noticed during my alpha testing of Eki Live is that there’s a psychological aspect to &lt;em&gt;needing&lt;/em&gt; to know where you are at any given time. This might affect certain people more than others. Whether it’s a nagging feeling of “oh no, did I miss my stop?” or something deeper about being moved through space in a way that humans only began to experience in the last couple hundred years, I still feel it’s difficult to completely give up my awareness as a passenger in a train, car, etc. Although this phenomenon may be real and may drive retention, it’s probably not enough of a top-of-mind motivator for someone want to download the app.&lt;/p&gt;

&lt;p&gt;There are a few &lt;em&gt;train native&lt;/em&gt; ways to stay updated on the train’s current location or otherwise know when to get off the train (I’ll ignore app-based tools for the moment).&lt;/p&gt;

&lt;h3 id=&quot;door-adjacent-live-updating-signage&quot;&gt;Door-adjacent live-updating signage&lt;/h3&gt;

&lt;p&gt;Most train cars have a full color LCD or at least dot-matrix screen near the doors that shows the current or next station on the line:&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-live-jr-door-lcd-sign.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Next station information in an LCD screen above the door on a JR Line car&quot; title=&quot;Next station information in an LCD screen above the door on a JR Line car&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Next station information in an LCD screen above the door on a JR Line car&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The screen cycles through several different information sets like all stops, platform maps, delay information, etc., so the exact information you need may be unavailable at the moment you need it (when quickly deciding whether to exit the train before the doors close).&lt;/li&gt;
  &lt;li&gt;The screen cycles through ~4 languages/scripts, so it may be unreadable to you for short periods.&lt;/li&gt;
  &lt;li&gt;The screen may not be visible from all standing/sitting positions, especially on crowded trains.&lt;/li&gt;
  &lt;li&gt;The text on the screen may be too small to read from your position.&lt;/li&gt;
  &lt;li&gt;Dot-matrix screens are particularly low-information density and hard to parse.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;audio-announcements&quot;&gt;Audio announcements&lt;/h3&gt;

&lt;p&gt;Announcements for the train’s current location are made over the loudspeakers in each car. They usually include an announcement for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The next station as soon as the doors close at the current station.&lt;/li&gt;
  &lt;li&gt;The approaching station within a couple hundred meters.&lt;/li&gt;
  &lt;li&gt;The current station as the doors are opening.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Depending on the line, these announcements are made consecutively in Japanese and English. They sometimes include a list of the connecting railway lines at a station, which side the doors will open on, a reminder of the rules of riding the train, or non-automated, urgent information about delays or accidents.&lt;/p&gt;

&lt;p&gt;Audio announcements are useful, but also have weak points:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Audio announcements are push-based not pull-based (i.e. not on-demand) – you need to pay attention at the right time or always be passively listening.&lt;/li&gt;
  &lt;li&gt;Audio announcements can be difficult to hear in noisy cars.&lt;/li&gt;
  &lt;li&gt;Audio announcements are inaudible when listening to music or spoken-word content on headphones.&lt;/li&gt;
  &lt;li&gt;Non-automated announcements (live from the conductor) can be especially difficult to understand.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;platform-signage&quot;&gt;Platform signage&lt;/h3&gt;

&lt;p&gt;When pulling into a station, the station’s name and its one or two adjacent stations are often written in various locations along the platform. Generally, whether or not you can see one of them as a rider is random depending on which train car you’re in, your relationship to a window facing the platform, and whether there are other passengers (on the train or waiting on the platform) blocking your view.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/station-detail-bashamichi-photo.jpg&quot; width=&quot;&quot; height=&quot;200&quot; alt=&quot;Platform signage on the opposite wall of the platform at Bashamichi station&quot; title=&quot;Platform signage on the opposite wall of the platform at Bashamichi station&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Platform signage on the opposite wall of the platform at Bashamichi station&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;timetable&quot;&gt;Timetable&lt;/h3&gt;

&lt;p&gt;If you somehow have access to the train’s timetable &lt;em&gt;and&lt;/em&gt; the train you’re riding stays true to that timetable through your arrival station, you could use your watch to know approximately when to get off the train.&lt;/p&gt;

&lt;p&gt;My other train-related app &lt;a href=&quot;/2024/07/27/eki-bright-tokyo-area-train-timetables/&quot;&gt;Eki Bright&lt;/a&gt; actually facilitates this method once you’ve selected your destination in the app. It shows your arrival time next to the current time as a Live Activity.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-arrival-time-dynamic-island.jpg&quot; width=&quot;&quot; height=&quot;200&quot; alt=&quot;Eki Bright&apos;s Live Activity showing a 15:11 arrival time at Naka-meguro station after setting up a DIY Route in the app&quot; title=&quot;Eki Bright&apos;s Live Activity showing a 15:11 arrival time at Naka-meguro station after setting up a DIY Route in the app&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Eki Bright&apos;s Live Activity showing a 15:11 arrival time at Naka-meguro station after setting up a DIY Route in the app&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;navigation-apps&quot;&gt;Navigation apps&lt;/h3&gt;

&lt;p&gt;Any number of popular navigation apps (Google Maps, Apple Maps, Jourdan Norikai Annai) will show you all kinds of information about your journey, both within the app and in other parts of the system via Live Activities, widgets, notifications, etc.&lt;/p&gt;

&lt;p&gt;However, the main downside to all of these apps is that you always need to explicitly tell the app that you’ve started a trip, where your destination is, and which specific departed train you’re riding. Or if you just want to see your current location on a map, you need to open the app and wait for the GPS to kick in.&lt;/p&gt;

&lt;p&gt;If you’re just taking a quick trip or you’re on a route you know well, it’s usually not worth the hassle to open a navigation app to search for and select your exact itinerary, especially if you’ve already departed.&lt;/p&gt;

&lt;h3 id=&quot;where-eki-live-shines&quot;&gt;Where Eki Live shines&lt;/h3&gt;

&lt;p&gt;Eki Live is especially useful for those everyday, routine rides. You don’t need to remember to open an app. You don’t need to bother selecting a route. It just appears when its relevant and disappears when it’s not. It adds another layer of security and awareness, especially when you’ve got your headphones in and are locked into reading an article or manga, watching a video, scrolling endlessly, or deep in a mobile game. It only needs a few pixels of otherwise unused screen real estate.&lt;/p&gt;

&lt;p&gt;Tourists may also find Eki Live useful, but I haven’t explored this use case fully. In theory, having another tool that automatically shows information about what railway line you’re riding could help tourists from getting lost. But especially with Eki Live’s current limitations on underground operation, thinking about another app may be a distraction.&lt;/p&gt;

&lt;h2 id=&quot;roadmap&quot;&gt;Roadmap&lt;/h2&gt;

&lt;p&gt;The first thing on the roadmap is to get a general feel for whether this idea connects with people:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Is the value-proposition presented in a way that makes people curious enough to download the app, set it up, and see it working as designed with their own eyes?&lt;/li&gt;
  &lt;li&gt;Do users start to rely on it enough to complain about its limitations?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This means marketing the app well enough to fill the top of the funnel.&lt;/p&gt;

&lt;p&gt;Assuming the above two points check out and the app concept is worth pursuing further, I’ll be working on two concurrent high-level goals:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Improve usability&lt;/strong&gt;: add more ways for users to customize the app if they so choose - snooze automatic tracking for some period of time; better support manually triggered tracking; add station arrival alarms; show more information in the app about the current railway; show estimated arrival times.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improve the tracking algorithm&lt;/strong&gt;: improve the train alighting detection; improve the differentiation between parallel railways; underground railway detection.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’d also like to incorporate some of the research done for Eki Live into Eki Bright as I originally intended. But that is lower priority until the concept of Eki Live is thoroughly proven or disproven.&lt;/p&gt;

&lt;h2 id=&quot;wrap-up&quot;&gt;Wrap up&lt;/h2&gt;

&lt;p&gt;I’m really looking forward to receiving feedback from everyone that tries out Eki Live. There are so many unique ways people get around Tokyo. Hopefully Eki Live can find its niche.&lt;/p&gt;
</description>
        <pubDate>Tue, 03 Jun 2025 07:27:00 -0500</pubDate>
        <link>https://twocentstudios.com/2025/06/03/eki-live-announcement/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2025/06/03/eki-live-announcement/</guid>
        
        <category>ekilive</category>
        
        <category>app</category>
        
        
      </item>
    
      <item>
        <title>Core Image Labo - Open Source iOS App for Core Image Experimentation</title>
        <description>&lt;p&gt;I wrote an iOS app called Core Image Labo for experimenting with &lt;a href=&quot;https://developer.apple.com/documentation/coreimage&quot;&gt;Core Image&lt;/a&gt; filters. It was a “weekend project” in service of a more fully-featured upcoming video-shooting app. I decided to clean it up and release on the App Store and as open source with an MIT license.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open source on &lt;a href=&quot;https://github.com/twocentstudios/coreimagelab&quot;&gt;GitHub - Core Image Labo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Available on the &lt;a href=&quot;https://apps.apple.com/us/app/core-image-labo/id6742433427&quot;&gt;App Store - Core Image Labo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/core-image-labo-marketing.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Marketing screenshots for Core Image Labo&apos;s v1.0 release&quot; title=&quot;Marketing screenshots for Core Image Labo&apos;s v1.0 release&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Marketing screenshots for Core Image Labo&apos;s v1.0 release&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You first set up a global input image (or use the default), and optionally a global background/target image (these are used for composite and transition filter types, respectively).&lt;/p&gt;

&lt;p&gt;Then you can add any number of CIFilters from the list of supported filters. I was most interested in filters with numerical inputs you could control via sliders, so that’s what I’ve implemented first.&lt;/p&gt;

&lt;p&gt;The other input types are slightly more complex (but very much reasonable) to model in UI like &lt;a href=&quot;https://developer.apple.com/documentation/coreimage/civector&quot;&gt;CIVector&lt;/a&gt; and &lt;a href=&quot;https://developer.apple.com/documentation/corefoundation/cgaffinetransform&quot;&gt;CGAffineTransform&lt;/a&gt;, and I don’t personally need to experiment with any of those filters at the moment, so I’ve held off on implementing support for them for v1.0.&lt;/p&gt;

&lt;p&gt;Finally, there are some simple tools for exporting the filtered image you see in the preview and a JSON file containing values for the filters used.&lt;/p&gt;

&lt;p&gt;I made an icon using Figma’s vector tools. Lately I’ve been using Blender to make icons in 3D, but I’ve been realizing that 3D-rendered images actually require some de-rendering to make them more illustrative and easier to read in the small pixel format of an app icon. For this side project, it was a lot faster to start from a 2D vector and render with simple shapes and color fills.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/core-image-labo-app-icon.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Core Image Labo&apos;s app icon created in Figma&quot; title=&quot;Core Image Labo&apos;s app icon created in Figma&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Core Image Labo&apos;s app icon created in Figma&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are already a few very robust tools for working with Core Image on macOS. Writing code helps me learn though, and it was nice to have my own sandbox to experiment with to (re)learn Core Image. I figured it might be useful to some other devs to have an open source base to work from in case they’re doing something unique that isn’t supported by the other commercial apps.&lt;/p&gt;

&lt;p&gt;If you’re a dev working with Core Image, give it a go and contribute a feature or a bug fix if you can.&lt;/p&gt;
</description>
        <pubDate>Tue, 25 Feb 2025 09:22:00 -0600</pubDate>
        <link>https://twocentstudios.com/2025/02/25/core-image-labo/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2025/02/25/core-image-labo/</guid>
        
        <category>coreimagelabo</category>
        
        <category>app</category>
        
        <category>ios</category>
        
        <category>apple</category>
        
        
      </item>
    
      <item>
        <title>8-bit Nails - Pixel Art Nail Diary</title>
        <description>&lt;p&gt;This week I released 8-bit Nails. It’s a light-hearted app for nail painting enthusiasts to express their creativity through pixel art and document their manicures.&lt;/p&gt;

&lt;p&gt;Download it &lt;a href=&quot;https://apps.apple.com/us/app/8-bit-nails/id6737764793&quot;&gt;on the App Store&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/8-bit-nails-marketing-images.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Marketing screenshots for 8-bit Nails on v1.0 release&quot; title=&quot;Marketing screenshots for 8-bit Nails on v1.0 release&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Marketing screenshots for 8-bit Nails on v1.0 release&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After you or a loved one gets their nails done IRL, jump into 8-bit Nails and create a matching set of nails with your own vision in pixel art style. Manicures can be simple or elaborate, and using your creativity to translate them into pixel art style is a fun challenge.&lt;/p&gt;

&lt;p&gt;The drawing tools are simple, but there are a few built-in helpers to selectively copy an individual nail across to other nails. You can customize the nail on each hand.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/8-bit-nails-drawing-tools.jpg&quot; width=&quot;&quot; height=&quot;450&quot; alt=&quot;8-bit Nails includes helper tools to eliminate the boring parts of painting&quot; title=&quot;8-bit Nails includes helper tools to eliminate the boring parts of painting&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;8-bit Nails includes helper tools to eliminate the boring parts of painting&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The system color picker is available, and the already used colors are easily accessible as a dynamic palette. And undo and redo functions are available for each nail.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/8-bit-nails-color-picker.jpg&quot; width=&quot;&quot; height=&quot;450&quot; alt=&quot;The color picker&quot; title=&quot;The color picker&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The color picker&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After you’ve finished pixel-fying your nails, they appear in the diary tagged with the current date. You can look back to see each of your nails over time.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/8-bit-nails-diary.jpg&quot; width=&quot;&quot; height=&quot;450&quot; alt=&quot;The main screen shows a diary of your nails with most recent at the top&quot; title=&quot;The main screen shows a diary of your nails with most recent at the top&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The main screen shows a diary of your nails with most recent at the top&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There’s also a full screen viewer when you want to show off your work in person.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/8-bit-nails-large-view.jpg&quot; width=&quot;&quot; height=&quot;450&quot; alt=&quot;View your nails in full screen&quot; title=&quot;View your nails in full screen&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;View your nails in full screen&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And finally, there’s a special shareable image version with an auto-generated background color. Save this to your camera roll or send it to friends.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/8-bit-nails-shareable-image.jpg&quot; width=&quot;&quot; height=&quot;250&quot; alt=&quot;An example shareable image&quot; title=&quot;An example shareable image&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;An example shareable image&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;My girlfriend is a nail enthusiast. She gets her nails painted every couple of weeks. I liked seeing what new designs she had and a budding idea came to me of having a painting app to keep track of them. While prototyping, I realized having a full suite of drawing tools and brushes was way too complicated and intimidating. But arbitrarily limiting the drawing resolution created a fun constraint, made it easier to paint on a smartphone-sized touch screen, and ensured that there was a soft-limit on the time it takes to get to the finish line.&lt;/p&gt;

&lt;p&gt;I finished a prototype version over a weekend, got it on Test Flight, and sent her an invite before the holidays. Over time, I’ve slightly improved the tools, fixed a few bugs, and added a few nice-to-have helpers. It was fun to see how both her and I interpreted her nails differently in pixel art style. Only after a couple rounds, I think each of us has gotten better at translating the pixel art style. Some of the more complex 3D designs she’s gotten IRL have been especially fun to try to paint in the app.&lt;/p&gt;

&lt;p&gt;When I started developing the app, I wasn’t planning on taking it beyond something for the two of us. But as I chipped away at features and slowly noticed how popular manicures were with those around me, I decided it’d be worthwhile to put the finishing touches on the app and release it publicly on the App Store.&lt;/p&gt;

&lt;p&gt;There are already a slew of manicure-related apps on the App Store. All fall into the category of games, photos for inspiration, or hyper-realistic painting simulators. Most are targeted towards young girls.&lt;/p&gt;

&lt;p&gt;Similarly, there are plenty of pixel art apps. On the casual side, there are paint-by-numbers apps. On the tools side, there are full pixel art suites with layers and other complex tools.&lt;/p&gt;

&lt;p&gt;I’m curious to see whether a cross between the two categories will find an audience.&lt;/p&gt;

&lt;h2 id=&quot;development&quot;&gt;Development&lt;/h2&gt;

&lt;p&gt;On the technical side, I’m using SwiftUI and no external frameworks. Since my original goal was a personal app, the code reflects that.&lt;/p&gt;

&lt;p&gt;Data for all nail sets are saved to one file. The data is saved as matrix of color values and drawn live in a SwiftUI &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Canvas&lt;/code&gt; View. Currently, the canvas is hard-coded to 10x16 pixels, but the code supports any resolution.&lt;/p&gt;

&lt;p&gt;I use the system color picker. Undo/redo is implemented as a stack of nail data for each nail in the set. I wanted to experiment with some custom transitions so I didn’t use any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sheet&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NavigationStack&lt;/code&gt; views this time; all views are layered in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZStack&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If the app starts to get traction, I’m planning on cleaning up the codebase. I realize with my last app Eki Bright I probably leaned a little too hard on the side of a clean codebase. If I want to continue on the indie dev path, I’ll need to keep optimizing for coding practices that facilitate a sustainable business, which means more up-front research, more marketing, more monetization, and more failing fast. All things that spending excess time in the codebase takes away from.&lt;/p&gt;

&lt;p&gt;I think it’d be fun to add a widget that shows your latest nails on your home screen. And allow customization of the nail shape. Besides that, I’m going to wait to see what real users are looking for.&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Feb 2025 06:22:00 -0600</pubDate>
        <link>https://twocentstudios.com/2025/02/17/8-bit-nails-pixel-art-nail-diary/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2025/02/17/8-bit-nails-pixel-art-nail-diary/</guid>
        
        <category>8bitnails</category>
        
        <category>app</category>
        
        
      </item>
    
      <item>
        <title>Eki Bright - Tokyo Area Train Timetables</title>
        <description>&lt;p&gt;My latest app is called Eki Bright or 駅ブライト in Japanese.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-app-icon.jpg&quot; width=&quot;&quot; height=&quot;300&quot; alt=&quot;Eki Bright app icon&quot; title=&quot;Eki Bright app icon&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Eki Bright app icon&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In short, it’s an app for quickly accessing offline station timetables and train timetables for railways in the Tokyo metropolitan area including Kanagawa, Chiba, and Saitama prefectures.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-v1-station-timetable.jpg&quot; width=&quot;&quot; height=&quot;650&quot; alt=&quot;The station timetable screen for Bashamichi station on the Minatomirai Line&quot; title=&quot;The station timetable screen for Bashamichi station on the Minatomirai Line&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The station timetable screen for Bashamichi station on the Minatomirai Line&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this series of posts I’ll talk about:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;This post&lt;/strong&gt; - the motivation behind the app and the solution I’ve begun to explore&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/2024/08/06/eki-bright-developing-the-app-for-ios/&quot;&gt;Eki Bright- Developing the App for iOS&lt;/a&gt;&lt;/strong&gt; - the high-level implementation details&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tokyo-area residents can download the app from the &lt;a href=&quot;https://apps.apple.com/app/%E9%A7%85%E3%83%96%E3%83%A9%E3%82%A4%E3%83%88/id6504702463&quot;&gt;App Store&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Eki Bright is currently closed source due to data licensing issues, but I’d like to open source it as some point once I get that aspect figured out.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;I’m a public transit (specifically train) fan. And Tokyo is easily one of the best cities in the world to get around by train.&lt;/p&gt;

&lt;p&gt;Due to the large market, there are already plenty of full featured transit apps – &lt;a href=&quot;https://maps.google.com/&quot;&gt;Google Maps&lt;/a&gt;, &lt;a href=&quot;https://www.apple.com/maps/&quot;&gt;Apple Maps&lt;/a&gt;, &lt;a href=&quot;https://www.navitime.co.jp/&quot;&gt;Navitime&lt;/a&gt;, &lt;a href=&quot;https://www.jorudan.co.jp/&quot;&gt;Jorudan&lt;/a&gt;, etc. And I’m a heavy user of all of them, especially Google Maps.&lt;/p&gt;

&lt;p&gt;However, after many years of use, I started to find some pain points.&lt;/p&gt;

&lt;p&gt;I got to know my daily trips well enough that I no longer needed to know all of the below information for each trip:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Which station I needed to walk to&lt;/li&gt;
  &lt;li&gt;How long it would take to walk to the station&lt;/li&gt;
  &lt;li&gt;Which railway I needed to use&lt;/li&gt;
  &lt;li&gt;Which platform I needed to wait at&lt;/li&gt;
  &lt;li&gt;Which train type I needed to get on&lt;/li&gt;
  &lt;li&gt;When the target train type would depart&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’d memorized 1-5, and only needed the departure times in order to optimize my trip.&lt;/p&gt;

&lt;p&gt;All of the other information requires opening Google Maps, waiting for it to load, entering a destination, occasionally entering an origin, waiting for the route data to load, scrolling, choosing the top result, scrolling.&lt;/p&gt;

&lt;p&gt;Then there were the times that I’d want to check departure times assuming I’d leave in half an hour. In that case I’d also have to manually enter a departure time or manually change the train in the helpful selection dropdown on the route page (surprisingly, a feature still not available on the web app).&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-gmaps-other-departures.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Adjusting the exact train within a chosen route is an essential step in using Google Maps&apos; routing&quot; title=&quot;Adjusting the exact train within a chosen route is an essential step in using Google Maps&apos; routing&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Adjusting the exact train within a chosen route is an essential step in using Google Maps&apos; routing&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There were also times where I knew more than Google Maps. For example, I know that on weekends, I can sometimes take a local or express train from Bashamichi station to Minatomirai station to catch a limited express train. This little maneuver can save 10-15 minutes into Shibuya. But this information is not easy to access with the Google Maps interface.&lt;/p&gt;

&lt;p&gt;Sure, for the most common situation I could head out from my apartment at whatever time was natural then wait on the platform for however long. But if I could see the next departures board at the station before I left my apartment, I could optimize my time better.&lt;/p&gt;

&lt;p&gt;Thus, the idea of Eki Bright was born.&lt;/p&gt;

&lt;h2 id=&quot;goals&quot;&gt;Goals&lt;/h2&gt;

&lt;p&gt;My north star for this app is, like I mentioned above, to have the same information I’d have while standing on the platform looking at the next departures board.&lt;/p&gt;

&lt;p&gt;The main principle required for that use case is simple: &lt;strong&gt;speed&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Every feature is evaluated against getting the amount of time it takes to see the departure timetable for a station optimized to near zero.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Offline data: no server roundtrip is required to fetch a station timetable, saving seconds on each tap, especially in slow network environments often expected when outside or underground. (The only reason this is even feasible is because trains in Japan are so rarely late.)&lt;/li&gt;
  &lt;li&gt;Bookmarked stations: the vast majority of trips are taken from the same stations: near home or work or a third place. Letting the user choose these and putting them on the home screen eliminates a step in getting them the information they need.&lt;/li&gt;
  &lt;li&gt;Nearby stations: for the next minority of trips, the user wants to see timetables for stations nearby their current location. This is trivial with location services (GPS).&lt;/li&gt;
  &lt;li&gt;Search: as a catch all for the remaining trips, stations can be found by text search (in romaji, hiragana, katakana, or kanji).&lt;/li&gt;
  &lt;li&gt;Widgets: for bookmarked stations, users can see timetables without even opening the app. The home screen or lock screen makes it trivial to see departure times at a glance.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-v1-home.jpg&quot; width=&quot;&quot; height=&quot;650&quot; alt=&quot;Nearby and bookmarked stations on the app home screen&quot; title=&quot;Nearby and bookmarked stations on the app home screen&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Nearby and bookmarked stations on the app home screen&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-v1-widgets.jpg&quot; width=&quot;&quot; height=&quot;650&quot; alt=&quot;Widgets make station timetables available at a glance&quot; title=&quot;Widgets make station timetables available at a glance&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Widgets make station timetables available at a glance&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My anti-goals – things I specifically did not want to include for fear of overcomplicating the interface and muddying the value proposition – are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Routing (i.e. inputting an origin and destination and calculating which trains to use)&lt;/li&gt;
  &lt;li&gt;Covering every city in Japan (or anywhere else in the world)&lt;/li&gt;
  &lt;li&gt;Covering other modes of public transit (e.g. buses)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Features I feel could potentially fit into the ethos of the app, but aren’t a high-priority:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Showing any information on a map&lt;/li&gt;
  &lt;li&gt;Live-updating departure times&lt;/li&gt;
  &lt;li&gt;Platform number information&lt;/li&gt;
  &lt;li&gt;Stair/elevator locations on the platform in relation to a train car&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One feature that wasn’t 100% in line with my north star but turned out to be so useful as to be prioritized for first release: train timetables. Tapping a departure time in the station timetable shows the full path of that particular train as a train timetable. This is useful for two reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Seeing your estimated arrival time at any destination on that line.&lt;/li&gt;
  &lt;li&gt;Seeing which stations that train stops at, in the case you’re not totally familiar with the local, express, etc. designations for that railway.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-v1-train-timetable.jpg&quot; width=&quot;&quot; height=&quot;650&quot; alt=&quot;train timetables show which stations a particular train will stop at, and at what times&quot; title=&quot;train timetables show which stations a particular train will stop at, and at what times&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;train timetables show which stations a particular train will stop at, and at what times&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I have many more ideas on how I can make this data alone even more useful to users without further complicating the app.&lt;/p&gt;

&lt;h2 id=&quot;one-example-of-ux-optimization-for-speed&quot;&gt;One example of UX optimization for speed&lt;/h2&gt;

&lt;p&gt;A conceptually-murky yet concretely defined &lt;em&gt;station&lt;/em&gt; in the app usually serves two directions unless it is a terminus.&lt;/p&gt;

&lt;p&gt;I made a decision early on for a &lt;em&gt;bookmark&lt;/em&gt; to represent the combination of a unique &lt;em&gt;station&lt;/em&gt; and a &lt;em&gt;direction&lt;/em&gt;. The reasoning being that it’s quite common for a passenger to only ride one direction from a particular station. For example, from my local station Bashamichi, I’m always going &lt;em&gt;inbound&lt;/em&gt; into Tokyo from my house and never &lt;em&gt;outbound&lt;/em&gt; back towards Motomachi.&lt;/p&gt;

&lt;p&gt;It would significantly slow me down if every time I had to select “inbound” or “outbound” each time I wanted to view the timetable for Bashamichi station.&lt;/p&gt;

&lt;p&gt;As the designer, I could put both timetables on one screen, however that would crowd the screen with useless (to me the user in my particular scenario) information.&lt;/p&gt;

&lt;p&gt;In the case where I do use both directions somewhat equally, it’s perfectly reasonable to have two bookmarks for the same station.&lt;/p&gt;

&lt;p&gt;I consider this well designed for the bookmarks use case. But for nearby stations and text search results users still need to select which direction they’re going before the app can display the correct timetable.&lt;/p&gt;

&lt;p&gt;My original design presented a station detail screen in this case. It showed no timetable data yet. The user had to select one of two (or one) direction first in order to view the timetable.&lt;/p&gt;

&lt;p&gt;This configuration almost immediately felt wrong during use. On one hand, users were less likely to make a mistake and misread the timetable for the opposite direction. On the other hand, it felt so slow having an intermediate step presented in order to see the timetable.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-v1-station-detail.jpg&quot; width=&quot;&quot; height=&quot;650&quot; alt=&quot;The deprecated Station Detail screen, looking alright but adding time and cognitive overhead&quot; title=&quot;The deprecated Station Detail screen, looking alright but adding time and cognitive overhead&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The deprecated Station Detail screen, looking alright but adding time and cognitive overhead&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the first release, I made two revisions to the UX:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I removed the station detail screen. After selecting a search result, the user would now be dropped directly into the timetable for the default direction for that station.&lt;/li&gt;
  &lt;li&gt;I added a quick swipe between that station’s direction’s timetables. Now users could quickly flip between the two directions with conceptually fewer steps.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-v1-station-timetable-swipe.gif&quot; width=&quot;&quot; height=&quot;650&quot; alt=&quot;Swipe between directions of a station or tap the segmented control at the bottom&quot; title=&quot;Swipe between directions of a station or tap the segmented control at the bottom&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Swipe between directions of a station or tap the segmented control at the bottom&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again, optimizing for speed.&lt;/p&gt;

&lt;p&gt;I could go further and silently save which direction for any given station the user last viewed. I could also redesign the station detail screen to be more useful to all use cases, whether that be quickly viewing either timetable or just getting metadata about the station of interest. Either way, I don’t want to compromise on the speed of getting a user to the information they’re looking for.&lt;/p&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;In my prototyping phase I did a dive into potential data sources, both online and offline, paid and free.&lt;/p&gt;

&lt;p&gt;In the current self-funding phase of my business, I heavily preferred not paying for data. This keeps my indefinite overhead low and means I have more flexibility in pricing in an already crowded and mature market.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://developer.odpt.org/&quot;&gt;Association for Open Data of Public Transportation&lt;/a&gt; aggregates and publishes data for transit systems across Japan. They also run a hackathon/contest for websites and apps built with their sourced data. This seemed like a reliable enough source for the data at the foundation of the app.&lt;/p&gt;

&lt;h2 id=&quot;prototyping&quot;&gt;Prototyping&lt;/h2&gt;

&lt;p&gt;I hacked together a widget that ingested the aggregated timetables for Bashamichi station (my most used station) on a weekday and showing them in a widget.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-v1-widget-proto.gif&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;One of the first widgets I hacked together&quot; title=&quot;One of the first widgets I hacked together&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;One of the first widgets I hacked together&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The goal at this point was to evaluate how well this implementation solved my hypothetical pain point. I used it for a couple weeks and, yes, I did feel it was a pretty useful supplement to my go-to Google Maps. I could swipe over on my home screen on my way out the door and see whether I need to hustle out to make the next limited express or whether I could take my time.&lt;/p&gt;

&lt;h2 id=&quot;productionizing&quot;&gt;Productionizing&lt;/h2&gt;

&lt;p&gt;At this point I had to decide whether to leave this as a &lt;em&gt;scratch-my-own-itch&lt;/em&gt; project, or whether it had potential as a full featured app I could offer of the App Store and support indefinitely. This isn’t an easy decision!&lt;/p&gt;

&lt;p&gt;In the end, at my level of business knowledge and sophistication, I made this decision from a selfish perspective:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How much did I personally want to use the most ideal version of this app?&lt;/li&gt;
  &lt;li&gt;How interested were my friends in the concept?&lt;/li&gt;
  &lt;li&gt;How motivated was I to work on this concept for weeks or months? Could I get it to the finish line?&lt;/li&gt;
  &lt;li&gt;How capable was I of doing the design on my own?&lt;/li&gt;
  &lt;li&gt;How much revenue did I need from the app to make it worthwhile?&lt;/li&gt;
  &lt;li&gt;How many monetizable features could I implement while still committing to simplicity and speed?&lt;/li&gt;
  &lt;li&gt;What other ideas was I more motivated to work on instead?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;making-sense-of-the-data&quot;&gt;Making sense of the data&lt;/h2&gt;

&lt;p&gt;I was by no means an expert on the intricacies of the vast Tokyo area train network. I’ve done my fair share of rides in the 6 years I’ve lived here, but vast majority of those rides were the same trains going to the same stations, with extracurricular trips being a novelty I quickly forgot the details of.&lt;/p&gt;

&lt;p&gt;Learning not only the train system as it exists in the real world, but also as it exists modeled in my chosen dataset was a process. A couple fun concepts I had to wrap my head around:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to model time, especially across midnight and logical day spans&lt;/li&gt;
  &lt;li&gt;How to choose a weekday, weekend, Saturday, or holiday schedule&lt;/li&gt;
  &lt;li&gt;How to internally and externally deal with train directions and destinations&lt;/li&gt;
  &lt;li&gt;How to deal with the one-off case of the one circle line (Yamanote)&lt;/li&gt;
  &lt;li&gt;How all these concepts were most naturally expressed in Japanese&lt;/li&gt;
  &lt;li&gt;How to deal with searching via multiple Japanese character sets&lt;/li&gt;
  &lt;li&gt;How to generalize train types like local, express, etc. over disparate railways&lt;/li&gt;
  &lt;li&gt;How to display colors and associate them with railway concepts&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-v1-db.png&quot; width=&quot;&quot; height=&quot;450&quot; alt=&quot;A peek into the SQLite database that powers Eki Bright&quot; title=&quot;A peek into the SQLite database that powers Eki Bright&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;A peek into the SQLite database that powers Eki Bright&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;internationalization&quot;&gt;Internationalization&lt;/h2&gt;

&lt;p&gt;My initial thought was that it’d be reasonable support both English and Japanese localizations at first release. Especially considering my dataset has pretty good English support.&lt;/p&gt;

&lt;p&gt;However, after getting into the details of the design, I realized it’d be a lot more pragmatic to optimize the interface for Japanese at first. Although I’m keen to help the low Japanese-proficiency community in Tokyo, realistically, the vast majority of my users will be comfortable enough with Japanese.&lt;/p&gt;

&lt;p&gt;In the next phase I’ll focus on getting internationalization complete on the interface layer of the app, then focus on the data layer. It will require some design thought because a lot of the time even English speakers will want access to both the English and Japanese station name, train type, etc. when out and looking for their train.&lt;/p&gt;

&lt;h2 id=&quot;icon&quot;&gt;Icon&lt;/h2&gt;

&lt;p&gt;Creating the app icon was the last step for Eki Bright. My intention for much of development was to create an anthropomorphized train character with a happy disposition, similar to the &lt;a href=&quot;/2023/10/30/count-biki-app-and-character-design/&quot;&gt;vampire rabbit mascot&lt;/a&gt; Count Biki.&lt;/p&gt;

&lt;p&gt;Step zero was buying a magazine with lots of various train photos.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-train-magazine.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;Luckily, train enthusiast magazines are not difficult to find in Japan&quot; title=&quot;Luckily, train enthusiast magazines are not difficult to find in Japan&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Luckily, train enthusiast magazines are not difficult to find in Japan&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Step one was sketching some trains, then iterating heavily on character designs.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-icon-sketchbook.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;My somewhat embarrassing sketchbook (blue pen/good sketches are by my friend Kazuyo)&quot; title=&quot;My somewhat embarrassing sketchbook (blue pen/good sketches are by my friend Kazuyo)&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;My somewhat embarrassing sketchbook (blue pen/good sketches are by my friend Kazuyo)&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Step two was 3D modeling.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-v1-icon-wip.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;3D design work in progress&quot; title=&quot;3D design work in progress&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;3D design work in progress&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which unfortunately ended up with me throwing in the towel regarding the character and pivoting into a more traditional icon.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/eki-bright-app-icon.jpg&quot; width=&quot;&quot; height=&quot;400&quot; alt=&quot;The final icon&quot; title=&quot;The final icon&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The final icon&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Although a cute character would be great, I realized it wasn’t a huge loss. Realistically, the app design is extremely utilitarian. A cute character on the book cover wouldn’t really reflect the contents very well.&lt;/p&gt;

&lt;p&gt;In the future, assuming a healthy adoption of the app, I’d love to do a full redesign that emphasizes playfulness alongside usefulness and a streamlined UX. At that point, I think it’d be reasonable to revisit the app icon design.&lt;/p&gt;

&lt;h2 id=&quot;monetization&quot;&gt;Monetization&lt;/h2&gt;

&lt;p&gt;Monetization on the App Store is still a mystery box to me. The entire market is relatively mature closing in on two decades on existence.&lt;/p&gt;

&lt;p&gt;At first release, I decided not to implement any in-app purchase or subscription at all. My focus up front is testing the market and ironing out the rough edges.&lt;/p&gt;

&lt;p&gt;I think there are 3 valid monetization paths (or some combination):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Themes: offer themes for the app and/or widgets that are each unlockable as a one-time purchase.&lt;/li&gt;
  &lt;li&gt;Subscription: make the app functionality severely limited outside a subscription: no nearby stations, no train timetables, only one bookmark/widget. Charge a low monthly/yearly rate to cover the ongoing development costs and data renewal costs.&lt;/li&gt;
  &lt;li&gt;One-time purchase: assume prospective users understand the value, and that I can cover my long-term costs by adding new users. The upside is pricing simplicity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One reason I’m not focused on monetization yet is that I’m not confident I can fill the top of the funnel.&lt;/p&gt;

&lt;p&gt;Getting anyone’s attention is often an insurmountable problem, especially when most people already have an okay-ish solution to the problem I’m trying to solve.&lt;/p&gt;

&lt;p&gt;Does anyone have space on their phone for another widget or app? Do they want to build new muscle memory and decide to open app A or app B depending on the situation? Is it a daily-use app that justifies an ongoing payment?&lt;/p&gt;

&lt;p&gt;Although I implemented tips in raw Store Kit for my last app &lt;a href=&quot;/2023/10/29/count-biki-japanese-numbers/&quot;&gt;Count Biki&lt;/a&gt;, and it’ll theoretically be less development this time around, it still requires enough work that could be better spent on marketing, especially while the top of the funnel is so small.&lt;/p&gt;

&lt;p&gt;All that’s to say that for now I’m going to focus on finding a reliable process to get users into the top of the funnel (the ever-reliable VC startup playbook, haha).&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I’m of course happy to see this little app make its way out into the world, and hopefully shave of a few seconds or minutes for train-hopping Tokyo residents. I’m a little biased, but I’ve found myself reaching for Eki Bright before Google Maps for a majority of my trips, even when multiple transfers are involved.&lt;/p&gt;

&lt;p&gt;It may simply be a naive sense of control Eki Bright gives me when bouncing around timetables, but regardless it’s surprisingly entertaining (rather than burdensome) to do my own simple route planning on the fly. And knowing I’m getting the absolute fastest trip from A to B.&lt;/p&gt;

&lt;p&gt;For developers, see you in the &lt;a href=&quot;/2024/08/06/eki-bright-developing-the-app-for-ios/&quot;&gt;next post&lt;/a&gt; for all the fun development details.&lt;/p&gt;
</description>
        <pubDate>Sat, 27 Jul 2024 09:02:00 -0500</pubDate>
        <link>https://twocentstudios.com/2024/07/27/eki-bright-tokyo-area-train-timetables/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2024/07/27/eki-bright-tokyo-area-train-timetables/</guid>
        
        <category>ekibright</category>
        
        <category>app</category>
        
        
      </item>
    
      <item>
        <title>Count Biki - Drill Japanese Numbers</title>
        <description>&lt;p&gt;I’m proud to present my latest iOS app, Count Biki.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-app-icon.png&quot; width=&quot;&quot; height=&quot;300&quot; alt=&quot;Count Biki app icon&quot; title=&quot;Count Biki app icon&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Count Biki app icon&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Count Biki a self-guided utility app for drilling numbers of all shapes and sizes in Japanese.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-listening-quiz.png&quot; width=&quot;&quot; height=&quot;650&quot; alt=&quot;The listening quiz screen: the screen you&apos;ll spend the most time on&quot; title=&quot;The listening quiz screen: the screen you&apos;ll spend the most time on&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The listening quiz screen: the screen you&apos;ll spend the most time on&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this series of posts I’ll talk about:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;This post&lt;/strong&gt; - the motivation behind the app and the solution I’ve begun to explore from the learner (user) perspective&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/2023/10/30/count-biki-app-and-character-design/&quot;&gt;Count Biki - App and Character Design&lt;/a&gt;&lt;/strong&gt; - the design process and specifics of creating the Count Biki character&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/2023/10/31/count-biki-developing-the-app-for-ios/&quot;&gt;Count Biki - Developing the App for iOS&lt;/a&gt;&lt;/strong&gt; - the high-level implementation details&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Japanese learners can download the app from the &lt;a href=&quot;https://apps.apple.com/us/app/count-biki/id6463796779&quot;&gt;App Store&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Like many of my other apps, Count Biki is open source on &lt;a href=&quot;https://github.com/twocentstudios/count-biki&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Counting to ten is one of the first things we learn in a foreign language. Each language has its own rules about how numbers combine to form larger numbers, how they combine with objects, and how they associate with the concept of time.&lt;/p&gt;

&lt;p&gt;When I first moved to Japan, every couple shops or restaurants I’d visit wouldn’t have a display at the cash register. The cashier would speak my 4 or 5 digit total aloud at native speed and I’d be lucky to get the first digit. I’d panic and fumble through the transaction, then forget about it until the next time. It was tough skill to practice.&lt;/p&gt;

&lt;p&gt;Japanese has unique readings for days of the month, e.g. July 1st, October 9th, etc. We learn them in the 101-level course, but in daily conversation I’d rarely need to recall each specific reading with native accuracy to get my point across. The knowledge gracefully slipped away as it usually does.&lt;/p&gt;

&lt;p&gt;Fast forward to now, and there are more language learning apps than ever. Even ones that help with numbers. However, I wanted a more streamlined way to practice all types of numbers, and I wanted to focus my development on Japanese and, as a learner, ensure I could practice the uniquely difficult parts of the language.&lt;/p&gt;

&lt;p&gt;In cases like dealing with money, the numbers are essentially random. Therefore, I didn’t want to rely on just hard coding a couple dozen entries. Computers are great at:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;generating random numbers&lt;/li&gt;
  &lt;li&gt;generating speech from text&lt;/li&gt;
  &lt;li&gt;generating spelled out text from numbers&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All the rules about our language systems are already encoded into our devices. I saw my role as being able to surfaces these tools in a convenient wrapper so we can drill the particular weak points of numbers endlessly.&lt;/p&gt;

&lt;h2 id=&quot;available-topics&quot;&gt;Available Topics&lt;/h2&gt;

&lt;p&gt;First in the UX flow, the learner choses a topic category. In the first release there are 4 categories: numbers, money, time durations, and dates &amp;amp; times.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-topic-categories.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;The topic categories screen&quot; title=&quot;The topic categories screen&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The topic categories screen&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Within each topic category, there are several topics.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-topic-money.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;The topics screen within the money category&quot; title=&quot;The topics screen within the money category&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The topics screen within the money category&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My goal in dividing up topics and categories was to remove the burden of configuring complicated sets of ranges and digit sliders from users. Of course, infinite customization can be added later. But for my first release, I wanted to make some guesses as to how the app can be immediately useful to many different skill levels, and also suggest scenarios that learners may not even recognize as unique and useful.&lt;/p&gt;

&lt;p&gt;In the numbers category, the app covers ranges of numbers mostly based on the number of significant digits you need to input. It’s rare to encounter someone reading you a 9-digit number with every digit as non-zero. People generally work with significant digits, maybe 3 or 4. So although there’s a “Master” category that covers 5 significant digits, the higher order drills for 100 million (億) and 10 trillion (兆) still only use 3 or 4 significant digits (with the remainder being zeros).&lt;/p&gt;

&lt;p&gt;In the money category, the app adds the 円 suffix and tailors the number ranges to cover common situations like the convenience store or restaurant. This category has been my most used so far.&lt;/p&gt;

&lt;p&gt;In the time durations category, the app covers the main groupings from seconds to years. Although I considered adding some topics to mix them, I’d like to do some more research to determine what the most common pairings in daily life situations are. This also adds complexity to the user input section (what’s the lowest friction way to enter in multiple unrelated numbers? The system date picker? One text box?).&lt;/p&gt;

&lt;p&gt;In the dates &amp;amp; times category, the app covers similar variants as the time durations. However, these represent individual moments in time. And there are many variants that are commonly used. For example, 24-hour time is very common in the language, as is AM/PM (午前、午後). Days of the month and months have several exception readings. The app even (for fun) has an experimental topic for converting Japanese calendar years to Gregorian calendar years (this is not easy!).&lt;/p&gt;

&lt;h2 id=&quot;listening-quiz&quot;&gt;Listening Quiz&lt;/h2&gt;

&lt;p&gt;At launch, the only quiz skill supported is listening.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-listening-quiz.png&quot; width=&quot;&quot; height=&quot;650&quot; alt=&quot;The listening quiz screen&quot; title=&quot;The listening quiz screen&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The listening quiz screen&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;behind the scenes, the app generates a number based on the topic as the question&lt;/li&gt;
  &lt;li&gt;the on-device text-to-speech engine speaks the question&lt;/li&gt;
  &lt;li&gt;the learner types their proposed answer in the input box&lt;/li&gt;
  &lt;li&gt;the learner taps the submit button to check their answer&lt;/li&gt;
  &lt;li&gt;if the answer is correct, the app plays some animations (Count Biki nods, confetti is thrown, and the device plays haptics) and the flow starts over&lt;/li&gt;
  &lt;li&gt;if the answer is incorrect, the app shows a red bar and play some other animations&lt;/li&gt;
  &lt;li&gt;the learner can tap the play button to replay the question&lt;/li&gt;
  &lt;li&gt;the learner can tap the show answer button to give up and show the answer&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The app keeps track of the number of questions answered correctly, incorrectly, and skipped. But as of the initial release, this data is only kept for the duration of the session and is not persisted.&lt;/p&gt;

&lt;p&gt;The only session mode available at launch is infinite drill mode. The app will keep generating questions forever, and it’s up to the learner to decide when they’d like to quit or a switch to a different topic. I chose this mode as the default intentionally, as it matches well with the ethos of the app: it’s a utility for additional practice. Although it adds some burden to the learner to choose their own curriculum, there’s power in “staying out of the learner’s way”). Eventually, I’d like to add a time attack mode and question attack mode to give learners the option of deciding on bounded study sessions in advance.&lt;/p&gt;

&lt;h2 id=&quot;voices&quot;&gt;Voices&lt;/h2&gt;

&lt;p&gt;The app uses iOS’s built-in text-to-speech (TTS) engine. Although the on-device TTS is not perfect and requires some user configuration, I felt it was a good starting point in keeping the complexity, cost, and potential failure points low.&lt;/p&gt;

&lt;p&gt;The iOS binary has a standard-quality TTS voice predownloaded for most languages. For Japanese, this voice is called Kyoko. As of iOS 17, there are 4 available Japanese voices: Kyoto, Otoya, Hattori, and O-ren. Kyoto and Otoya have a separate “enhanced” quality voice.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-accessibility-settings-voices.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;iOS spoken content voices accessibility settings in iOS 17&quot; title=&quot;iOS spoken content voices accessibility settings in iOS 17&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;iOS spoken content voices accessibility settings in iOS 17&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As of the initial release of Count Biki, the learner can choose any of the voices they’ve downloaded from the iOS settings to use in their listening quiz. Using alternate voices requires going deep into the iOS accessibility settings and downloading ~70MB voice files. We’ve included instructions within the Count Biki settings that show the step-by-step of how to add voices, but unfortunately there’s no way to make this process more automated yet.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-get-more-voices.png&quot; width=&quot;&quot; height=&quot;550&quot; alt=&quot;The in-app explainer for how to download voices from iOS system settings&quot; title=&quot;The in-app explainer for how to download voices from iOS system settings&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The in-app explainer for how to download voices from iOS system settings&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As AI-powered TTS engines become commodities over the next few years, I’ll be looking to increase the variety, accuracy, and aesthetics of the TTS component of the app.&lt;/p&gt;

&lt;p&gt;The iOS TTS engine does include APIs for changing the speaking rate and pitch. I find the default rate to be a little quick. In the Count Biki app settings the app includes sliders for modifying both the rate and pitch. The extreme settings are definitely extreme, but I didn’t want to artificially limit them for the initial release.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-voice-settings.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;Voice settings within the session settings screen&quot; title=&quot;Voice settings within the session settings screen&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Voice settings within the session settings screen&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;quiz-results&quot;&gt;Quiz results&lt;/h2&gt;

&lt;p&gt;Generally, seeing the results of a quiz is important for a few reasons. You want to spend more time practicing questions you’ve missed. You want an overview of your progress over time in mastering a topic.&lt;/p&gt;

&lt;p&gt;Going along with the utility ethos of the app, I’ve only included the most basic quiz results; the results are displayed inline on the listening quiz screen and  slightly more detailed on the session settings screen.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-session-results.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;Results as displayed on the session settings screen&quot; title=&quot;Results as displayed on the session settings screen&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Results as displayed on the session settings screen&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As much as I wanted to create an elaborate results system for the initial release, the more I considered it, the more work I realized it would take to make it useful and not get in the way. It also seemed like it’d end up as an endless rabbit hole of complexity that would make implementing other skills and topics more difficult.&lt;/p&gt;

&lt;p&gt;The naive solution of showing a big list or correct or incorrect questions doesn’t seem like it’d add a lot of value on its own. I’d rather spend more time considering a few key metrics that I could extract for the user that are 1. understandable at a glance and 2. immediately actionable. I’m regretfully offloading the burden of progress tracking onto the user while the app is still taking shape.&lt;/p&gt;

&lt;h2 id=&quot;app-info&quot;&gt;App info&lt;/h2&gt;

&lt;p&gt;Most apps have an info/settings page that collects all kinds of disparate non-essential functions. Count Biki will probably end up with its mutable settings spread out across a few different contexts, so this page is more about tips, support, and legal info.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-info-screen-top.png&quot; width=&quot;&quot; height=&quot;650&quot; alt=&quot;The info screen&quot; title=&quot;The info screen&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The info screen&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;in-app-purchases&quot;&gt;In app purchases&lt;/h2&gt;

&lt;p&gt;When I was first charging for my apps, the App Store didn’t have in app purchases. It was actually still the era of the “lite version” - a completely separate binary and listing on the App Store with only a subset of features that would link users to pay for and download the full version.&lt;/p&gt;

&lt;p&gt;Since I’m starting to dip my toes in the water in building a self-sustaining independent app business, but my app isn’t at subscription-level value yet (in my opinion), I figured I’d try out the tip jar payment model. The plan is that almost all features are free upon first release, but features added in the future will require a pay-what-you-want-once upgrade. I called this “Transylvania Tier” as a nod to the vampire theme.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-transylvania-tier.png&quot; width=&quot;&quot; height=&quot;750&quot; alt=&quot;The tip-jar screen for unlocking &apos;Transylvania Tier&apos;&quot; title=&quot;The tip-jar screen for unlocking &apos;Transylvania Tier&apos;&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The tip-jar screen for unlocking &apos;Transylvania Tier&apos;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There is one unlockable feature: alternate app icons.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/count-biki-v1-app-icons.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;The tip-jar screen for unlocking &apos;Transylvania Tier&apos;&quot; title=&quot;The tip-jar screen for unlocking &apos;Transylvania Tier&apos;&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The tip-jar screen for unlocking &apos;Transylvania Tier&apos;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I’m hoping Count Biki will prove to be an invaluable addition to learners’ study materials. There are plenty more topics and features that complement the theme and goal of the app that I’ll continue to chip away at over time and as I get more feedback from learners.&lt;/p&gt;

&lt;p&gt;Thanks for reading this little checkpoint summary, and if you’re a Japanese learner or developer, please check out the app on the &lt;a href=&quot;https://apps.apple.com/us/app/count-biki/id6463796779&quot;&gt;App Store&lt;/a&gt; or the source on &lt;a href=&quot;https://github.com/twocentstudios/count-biki&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next post in the series is &lt;a href=&quot;/2023/10/30/count-biki-app-and-character-design/&quot;&gt;Count Biki - App and Character Design&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sun, 29 Oct 2023 14:49:00 -0500</pubDate>
        <link>https://twocentstudios.com/2023/10/29/count-biki-japanese-numbers/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2023/10/29/count-biki-japanese-numbers/</guid>
        
        <category>app</category>
        
        <category>countbiki</category>
        
        
      </item>
    
      <item>
        <title>Goalie - A Bespoke macOS App for Time Tracking</title>
        <description>&lt;p&gt;I self-released a bespoke time tracking app for macOS called Goalie in July 2023. I also &lt;a href=&quot;https://github.com/twocentstudios/goalie&quot;&gt;open sourced&lt;/a&gt; it. It looks like this:&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/goalie-main.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;Today view&quot; title=&quot;Today view&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Today view&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Download the latest version directly from the GitHub &lt;a href=&quot;https://github.com/twocentstudios/goalie/releases&quot;&gt;releases page&lt;/a&gt; if you want to try it out.&lt;/p&gt;

&lt;p&gt;In this post I’ll give some background behind the app and a discuss a few notes about the implementation, and talk about how I created the icon.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;I created Goalie as my first bite-sized project after &lt;a href=&quot;/2023/10/18/cookpad-retrospective/&quot;&gt;leaving Cookpad&lt;/a&gt; at the end of June 2023. Since I’d no longer be immersed in the Japanese language at the office every day, I wanted to make sure I was putting in a fair amount of time studying Japanese on my own.&lt;/p&gt;

&lt;p&gt;I could have used one of a million time tracking apps focused on invoicing or Pomodoro, or even macOS’s built-in Screen Time feature. But this felt like a great opportunity to make a focused piece of software to my own specifications. I also wanted to do a SwiftUI-based macOS app to see where the framework was currently at. My past experience and impression from reading other developers’ thoughts was that SwiftUI on macOS has always significantly lagged behind iOS.&lt;/p&gt;

&lt;h2 id=&quot;how-does-it-work&quot;&gt;How does it work?&lt;/h2&gt;

&lt;h3 id=&quot;start-and-stop-the-timer&quot;&gt;Start and stop the timer&lt;/h3&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/goalie-main.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;Today view&quot; title=&quot;Today view&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Today view&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Clicking start starts a new “session”. Stopping ends the session.&lt;/p&gt;

&lt;p&gt;Once you reach your daily goal, the goal button lights up green.&lt;/p&gt;

&lt;h3 id=&quot;optionally-set-a-goal&quot;&gt;Optionally set a goal&lt;/h3&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/goalie-set-goal.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;Goal view&quot; title=&quot;Goal view&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Goal view&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Click the goal button and drag the slider to set your daily goal. Changing the goal sets the goal from that day forward until you change it again. It doesn’t affect past goals.&lt;/p&gt;

&lt;h3 id=&quot;show-and-edit-daily-sessions&quot;&gt;Show and edit daily sessions&lt;/h3&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/goalie-main-expanded.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;Today view with sessions expanded&quot; title=&quot;Today view with sessions expanded&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Today view with sessions expanded&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Click the “sessions today” button to show other sessions from the day. If you want to remove one, click the “x” button to its left.&lt;/p&gt;

&lt;h3 id=&quot;show-history&quot;&gt;Show history&lt;/h3&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/goalie-history.png&quot; width=&quot;&quot; height=&quot;350&quot; alt=&quot;History view&quot; title=&quot;History view&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;History view&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Click the calendar to show a weekly summary of your goal. Click the arrows in the header to page back through the history by weeks.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Goalie targets the SwiftUI 4 APIs (i.e. macOS 13, iOS 16).&lt;/p&gt;

&lt;p&gt;I decided to go with a vanilla SwiftUI architecture that separates the view and store. The one new thing I wanted to try was pointfreeco’s &lt;a href=&quot;https://github.com/pointfreeco/swift-dependencies&quot;&gt;swift-dependencies&lt;/a&gt; library to control my dependency layer.&lt;/p&gt;

&lt;p&gt;Although this kind of app is often implemented as a &lt;a href=&quot;https://developer.apple.com/design/human-interface-guidelines/the-menu-bar#Menu-bar-extras&quot;&gt;menu bar extra&lt;/a&gt;, I preferred a simple app window I can keep off to the corner of my big monitor. My menu bar is already too full. As an app window I can keep it in my peripheral vision or hide it behind other windows easily. If I were to formally release this app for a more general audience, I’d probably add a menu bar extra though.&lt;/p&gt;

&lt;p&gt;I used a ViewData abstraction to encapsulate some of the simple synchronous logic that turns raw model data into properties ready for the view to display directly.&lt;/p&gt;

&lt;p&gt;For styling, I tried to use UIKit system colors and font styles as best I could to get a classic look and dark mode support. I picked a bluish purple for the tint color. I kept branding to an absolute minimum.&lt;/p&gt;

&lt;p&gt;For persistence, I made my model layer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Codable&lt;/code&gt; and save it to the application support directory. The app currently only supports one “topic”, but I’ve left the door open to make it a proper document-based app that allows having multiple topics going at the same time. At the moment that goes beyond my personal use case, so I haven’t explored it any more.&lt;/p&gt;

&lt;h3 id=&quot;dates-and-times&quot;&gt;Dates and times&lt;/h3&gt;

&lt;p&gt;Dates and times and calendars are well known to &lt;a href=&quot;https://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time&quot;&gt;cause headaches&lt;/a&gt; for developers.&lt;/p&gt;

&lt;p&gt;Even for what I considered to be as close to a trivial time tracking code as one could write, I still immediately ran into gotchas. Especially when I went to write the history code.&lt;/p&gt;

&lt;p&gt;I think any time you start to divide hours up into days with a beginning and end, you’ve got use a calendar and time zones, of which there are many. When you want to divide days up into weeks, the problems compound.&lt;/p&gt;

&lt;p&gt;It doesn’t help that many of Apple’s underlying APIs for calendars silently use the user’s current locale and more specific localization settings under the hood. This usually great, but makes it incredibly difficult to introduce complexity to your codebase slowly, and also easily test each locale.&lt;/p&gt;

&lt;p&gt;I made significant use of the new Foundation formatter APIs, which, in theory, wield like a Swiss Army Knife. But in practice, there’s no official documentation, the source is completely obscured by the heavy use of generics, and you still need detailed background knowledge of how the several dozen locales affect the results. Luckily &lt;a href=&quot;https://goshdarnformatstyle.com/&quot;&gt;Gosh Darn Format Style!&lt;/a&gt; helped take some of the edge off.&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;/// An example of calculating and displaying the duration of an ongoing session:&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;now&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;startOfDay&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calendar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;startOfDay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Enumerates all sessions to calculate the total number of seconds logged for the current day up to now.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;totalIntervalToday&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;totalIntervalBetween&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;startOfDay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Convert to `Duration` type for formatting.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;duration&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totalIntervalToday&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Produces something like 02:15:12 for a number of seconds.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// (This should probably explicitly reference `locale`).&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;formatted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;pattern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
            &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hourMinuteSecond&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;nv&quot;&gt;padHourToLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;nv&quot;&gt;fractionalSecondsLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;nv&quot;&gt;roundFractionalSeconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;up&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;What this all means is that this code is unfortunately not well tested in all locales, which is the main reason I haven’t pushed for a wide release.&lt;/p&gt;

&lt;h3 id=&quot;data-modeling&quot;&gt;Data modeling&lt;/h3&gt;

&lt;p&gt;I chose to model a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Topic&lt;/code&gt; (the highest level model) as such:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Topic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Equatable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Identifiable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Codable&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;UUID&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// non-nil when a session is active&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;activeSessionStart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?&lt;/span&gt; 

    &lt;span class=&quot;c1&quot;&gt;// assume sorted past to future&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;sessions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;IdentifiedArrayOf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; 

    &lt;span class=&quot;c1&quot;&gt;// assume sorted past to future, no two goals on the same day&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;goals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;IdentifiedArrayOf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Goal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This data is relatively normalized in that I have to parse it a bit to configure the view layer. Let me show the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Session&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Goal&lt;/code&gt; models before explaining further.&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Equatable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Identifiable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Codable&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;UUID&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Date&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Date&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Goal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Equatable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Identifiable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Codable&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;UUID&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// always normalized to the start of a day (this has known issues with time zone changing)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Date&lt;/span&gt; 

    &lt;span class=&quot;c1&quot;&gt;// nil intentionally unsets a goal, always &amp;gt; 0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;TimeInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;?&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The main consequence of logging only the start and end date of a session instead of updating the model to count up seconds is that the SwiftUI view updates independently from the model using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TimelineView&lt;/code&gt; primitive. If the computer is put to sleep or hidden, there won’t be any error in the model timekeeping. A downside is that using TimelineView offloads some necessary logic to the view layer, especially with date formatting. It’s a trade off.&lt;/p&gt;

&lt;p&gt;This setup generally uses the least amount of data necessary to capture the full state and history of the system. I calculate the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Goal&lt;/code&gt; moving forward in time indefinitely or until the next &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Goal&lt;/code&gt; object is logged. This adds some calculation burden, but it more efficient in storage. Of course, if users were allowed to, for example, insert a new goal for just one day, it would make that insertion logic slightly more complicated in the code.&lt;/p&gt;

&lt;p&gt;Solving the user-can-move-time-zones problem is tough enough that I haven’t bothered. There are a couple UX questions about the “correct” behavior that aren’t immediately obvious to me.&lt;/p&gt;

&lt;h3 id=&quot;start-of-day&quot;&gt;Start of day&lt;/h3&gt;

&lt;p&gt;The concept of “start of the current day” is important since the count up timer is always calculated based on that point in time.&lt;/p&gt;

&lt;p&gt;I thought macOS would have some sort of globally posted notification for when the day rolls over, but I couldn’t find one. One lingering issue is that my workaround code for updating that very important start-of-day value wasn’t running reliably. I’m much more familiar with the iOS app lifecycle than the macOS lifecycle so it’s unclear to me whether it’s an alarm setting error on my part, or whether the app just isn’t firing the alarm and updating the date when it’s not the key window. Either way, I added a workaround in v1.0.1 to simply ensure the start-of-day value is updated when the window is foregrounded. I recognize it’s not a perfect fix, but one step at a time.&lt;/p&gt;

&lt;h3 id=&quot;deployment&quot;&gt;Deployment&lt;/h3&gt;

&lt;p&gt;I haven’t set up fastlane, CI, or even Xcodegen for this project. But it’s surprisingly straightforward to do your own manual app build, signing, and distribution through GitHub. Next on the list would be a proper App Store release or Sparkle integration.&lt;/p&gt;

&lt;h2 id=&quot;designing-the-icon&quot;&gt;Designing the icon&lt;/h2&gt;

&lt;p&gt;I was initially tempted to use Stable Diffusion to create a low-effort app icon. However, lately I’ve been dabbling in 3D modeling with Blender and I decided to see how far I could get using it to create an icon.&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/goalie-icon.png&quot; width=&quot;&quot; height=&quot;&quot; alt=&quot;App icon&quot; title=&quot;App icon&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;App icon&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It turns out, pretty far! I actually like this icon a lot and it didn’t take so long to put together.&lt;/p&gt;

&lt;p&gt;If I were to log some more hours on it, I’d make it give it more of a macOS vibe by removing the heavy white border and try overhanging some of the foreground elements. Also I’d make some more custom low-res versions. Overall though, I find it easy to recognize in my dock when I need it.&lt;/p&gt;

&lt;p&gt;Here are some more WIP shots of the design process:&lt;/p&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/goalie-icon-design-01.png&quot; width=&quot;&quot; height=&quot;&quot; alt=&quot;Modeling the goal&quot; title=&quot;Modeling the goal&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Modeling the goal&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/goalie-icon-design-02.png&quot; width=&quot;&quot; height=&quot;&quot; alt=&quot;The grass and camera position&quot; title=&quot;The grass and camera position&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;The grass and camera position&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/goalie-icon-design-03.png&quot; width=&quot;&quot; height=&quot;&quot; alt=&quot;Setting up the shaders&quot; title=&quot;Setting up the shaders&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Setting up the shaders&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This app has been working well for me over the past month or two. It’s been great having some extra time to make some scratch-your-own-itch apps. There are plenty of ways I could improve the app, but for now, I’m satisfied with where it’s at and hope it continues to serve me well.&lt;/p&gt;

&lt;p&gt;Here’s one more link to the source and releases: &lt;a href=&quot;https://github.com/twocentstudios/goalie&quot;&gt;twocentstudios/goalie&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Fri, 20 Oct 2023 14:20:00 -0500</pubDate>
        <link>https://twocentstudios.com/2023/10/20/goalie-time-tracking-macos/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2023/10/20/goalie-time-tracking-macos/</guid>
        
        <category>apple</category>
        
        <category>ios</category>
        
        <category>app</category>
        
        
      </item>
    
      <item>
        <title>Photo/Phono - Your Photos as Music</title>
        <description>&lt;p&gt;My latest app is called Photo/Photo. It’s an iOS app that turns your photos into original musical compositions. It’s an experiment in procedurally/algorithmically generated music.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You can download it from the App Store &lt;a href=&quot;https://itunes.apple.com/us/app/photo-phono/id1202606014?mt=8&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Using Photo/Phono is pretty simple.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pick which “composer” module you want to use to transform your photo.&lt;/li&gt;
  &lt;li&gt;Select a photo from your photo library.&lt;/li&gt;
  &lt;li&gt;Listen to a brand new composition composed from the data in your photo.&lt;/li&gt;
  &lt;li&gt;When you find a photo and composition pair you like, share it.&lt;/li&gt;
&lt;/ul&gt;

&lt;video src=&quot;/images/photophono-preview_video.mov&quot; controls=&quot;&quot; preload=&quot;none&quot; poster=&quot;/images/photophono-preview_video_poster.png&quot;&gt;&lt;/video&gt;

&lt;div class=&quot;caption-wrapper&quot;&gt;&lt;img class=&quot;caption&quot; src=&quot;/images/photophono-screens.png&quot; width=&quot;&quot; height=&quot;&quot; alt=&quot;Walkthrough&quot; title=&quot;Walkthrough&quot; /&gt;&lt;div class=&quot;caption-text&quot;&gt;Walkthrough&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;rules&quot;&gt;Rules&lt;/h3&gt;

&lt;p&gt;There are a few basic rules I wanted to abide by while designing the app:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Compositions are deterministic.&lt;/strong&gt; A particular photo will always return the same composition. There are no outside elements of randomness used to create a composition.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compositions are unique.&lt;/strong&gt; This is more of a guideline. No two photos should have the same exact composition. I couldn’t quite guarantee this rule, so in practice you may find visually similar photos may have similar sounding compositions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-does-the-transformation-process-work&quot;&gt;How does the transformation process work?&lt;/h3&gt;

&lt;p&gt;An image is selected. Time to get to work.&lt;/p&gt;

&lt;p&gt;First, the image is decomposed into various representations of numerical data. Some examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the average red color value of the whole image&lt;/li&gt;
  &lt;li&gt;the average saturation value of one row of pixels&lt;/li&gt;
  &lt;li&gt;the number of faces in the image&lt;/li&gt;
  &lt;li&gt;the dominant colors of the image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This step gives us plenty of raw data to work with. (I go into more detail in &lt;a href=&quot;http://twocentstudios.com/2016/10/11/images-into-music-deconstruction/&quot;&gt;this post&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Next, for each instrument, the algorithm uses the photo data to make decisions on how notes should be created. A note can have a location in the piece, a pitch, a loudness, and a duration. That’s a lot of decisions that need to be made.&lt;/p&gt;

&lt;p&gt;Notes are created in an custom intermediate format that’s a little easier to work with. This allows the algorithm to more easily concatenate bars or sections, with less complexity than is required by the MIDI specification. The basic building block looks like this:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;NoteEvent&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;NoteNumber&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;NoteVelocity&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Beats&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Beats&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now that we have an representation of the piece that contains the notes in bars in sections for each instrument, it can be transformed into MIDI data. But there’s still one more step before our MIDI data will be audible.&lt;/p&gt;

&lt;p&gt;Each composer module is paired with a file called a &lt;a href=&quot;https://en.wikipedia.org/wiki/SoundFont&quot;&gt;SoundFont&lt;/a&gt;. Since MIDI data is just instructions on how to play the music, a SoundFont is needed to actually generate the sound waves we hear. It’s similar to a piece of sheet music needing an instrument to be played on. The sheet music is just instructions, and it could be played on a grand piano, harpsichord, an upright piano, etc.&lt;/p&gt;

&lt;p&gt;Finally, our MIDI data and SoundFont are handed off to the system’s audio frameworks and played back through the speakers or headphones.&lt;/p&gt;

&lt;h3 id=&quot;but-how-does-the-composing-module-actually-create-the-piece&quot;&gt;But how does the composing module actually &lt;em&gt;create&lt;/em&gt; the piece?&lt;/h3&gt;

&lt;p&gt;I go into more technical detail in &lt;a href=&quot;http://twocentstudios.com/2016/10/12/image-into-music-transformation/&quot;&gt;this post&lt;/a&gt;, but here is an overview and a bit of background.&lt;/p&gt;

&lt;p&gt;Each composer algorithm works differently. I wanted each algorithm to be free to explore using a palate of data and a canvas of MIDI so to speak. The only two rules are the ones mentioned above regarding being deterministic and being unique.&lt;/p&gt;

&lt;p&gt;However, one of my other goals for this project was for the music to be a reflection of the artist creating the algorithm. It was a challenge to myself as a composer and a student of music theory to ask myself: when I sit down to write music, how do I actually &lt;em&gt;do&lt;/em&gt; it? Would it be possible to enumerate all the different ways I know how to create aesthetically pleasing music and encode those into the computer?&lt;/p&gt;

&lt;p&gt;I can’t say that the two composer algorithms I’ve created at first launch are the pinnacle of my abilities. But through the process of designing these, I’ve already learned a lot. And I have a handful of ideas of how I’d like to approach making the next one.&lt;/p&gt;

&lt;p&gt;(If you’re a composer interested in designing your own algorithm for Photo/Photo, please get in touch!)&lt;/p&gt;

&lt;p&gt;For the above reasons and more, I ultimately decided against exploring machine-learning for this project. There’s lots of amazing research in this field (I have a few links below).&lt;/p&gt;

&lt;h3 id=&quot;technical-notes&quot;&gt;Technical notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;I wrote about a few technical challenges creating the sharing feature. Here are posts about &lt;a href=&quot;http://twocentstudios.com/2017/02/20/bouncing-midi-to-audio-on-ios/&quot;&gt;bouncing MIDI to audio&lt;/a&gt; and &lt;a href=&quot;http://twocentstudios.com/2017/02/20/creating-a-movie-with-an-image-and-audio-on-ios/&quot;&gt;creating a movie with an image and audio&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The UI is only a few screens, but it gave me a chance to experiment with an MVVM+C (Model-View-ViewModel + Coordinator) architecture. I found a couple techniques that worked out well that I’m going to reproduce on future projects.&lt;/li&gt;
  &lt;li&gt;I used &lt;a href=&quot;https://github.com/ReactiveX/RxSwift&quot;&gt;RxSwift&lt;/a&gt; instead of &lt;a href=&quot;https://github.com/ReactiveCocoa/ReactiveSwift&quot;&gt;ReactiveSwift&lt;/a&gt; on this project. ReactiveSwift was under heavy development when I was getting started, and I also wanted to get some experience with the RxSwift API. I’m probably going to move back to ReactiveSwift for my next project though.&lt;/li&gt;
  &lt;li&gt;This project is 100% Swift. I love the language itself and especially look forward to writing it in a few years once the language and toolchain have stabilized. Some souring experiences were: going through a somewhat painful Swift 2.2 to 3.0 transition half way through the project, dealing with the frequent loss of autocomplete and syntax highlighting, and slow build times even with my reasonably small codebase.&lt;/li&gt;
  &lt;li&gt;I did all the design work myself, starting with some drawings in my notebook, then moving to a Sketch file. I tried to use some of the newer, bolder iOS 10 aesthetic found in Music.app and News.app.&lt;/li&gt;
  &lt;li&gt;This is the first project I’ve used Auto Layout exclusively (albeit using the &lt;a href=&quot;https://github.com/jmfieldman/Mortar&quot;&gt;Mortar&lt;/a&gt; helper library). I still find that working directly with frames is more predictable in how long it will take to create a complex layout, whereas implementing a layout using Auto Layout can range from trivial to impossible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;wrap-up&quot;&gt;Wrap Up&lt;/h3&gt;

&lt;p&gt;Starting off with the simple idea, “How would you turn a photo into music?” led to solving a lot of interesting problems and learning a lot (while getting to scratch a musical itch). I hope you enjoy the app!&lt;/p&gt;

&lt;p&gt;Here are some links:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://itunes.apple.com/us/app/photo-phono/id1202606014?mt=8&quot;&gt;Photo/Phono on the App Store&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.flow-machines.com/&quot;&gt;Sony Computer Science Laboratories - Music Research&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tones.wolfram.com/&quot;&gt;Wolfram Tones&lt;/a&gt; - “An Experiment in a New Kind of Music”&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dmitri.tymoczko.com/whatmakesmusicsoundgood.html&quot;&gt;What Makes Music Sound Good?&lt;/a&gt; - Research by Dmitri Tymoczko&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Feedback is welcome! I’m &lt;a href=&quot;https://twitter.com/twocentstudios&quot;&gt;@twocentstudios&lt;/a&gt; on Twitter.&lt;/p&gt;
</description>
        <pubDate>Fri, 24 Feb 2017 01:10:39 -0600</pubDate>
        <link>https://twocentstudios.com/2017/02/24/photo-phono-your-photos-as-music/</link>
        <guid isPermaLink="true">https://twocentstudios.com/2017/02/24/photo-phono-your-photos-as-music/</guid>
        
        <category>apple</category>
        
        <category>ios</category>
        
        <category>photophono</category>
        
        <category>app</category>
        
        
      </item>
    
  </channel>
</rss>
